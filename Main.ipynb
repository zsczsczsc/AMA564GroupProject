{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MsKZpPPYs0kS"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from sklearn.metrics import classification_report, roc_auc_score, f1_score,accuracy_score,average_precision_score,recall_score,auc\n",
        "from scipy.stats import entropy\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "from operator import itemgetter\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from scipy.io import loadmat\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import scipy\n",
        "from scipy import sparse as sp\n",
        "import pickle\n",
        "import copy as cp\n",
        "from collections import defaultdict\n",
        "import random\n",
        "DATAPATH='/content/drive/MyDrive/Amazon.mat'# Path of Amazon.mat file\n",
        "prefix_1 = '/content/drive/MyDrive/' # Path to store temporary output files\n",
        "prefix_2 = '/content/drive/MyDrive/' # Folder that stores Amazon.mat file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cu118.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoVlShSv4QOV",
        "outputId": "d9a72b7b-a124-4732-c2f5-ace2c7651a9d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.9/dist-packages (2.1.1+pt20cu118)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.9/dist-packages (0.6.17+pt20cu118)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy->torch-sparse) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGCHGcSbicJ-",
        "outputId": "df50d2c4-d3bf-4cc1-e489-f8edd18a3b26"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.9/dist-packages (2.3.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (2.27.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (5.9.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch_geometric) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (2.0.12)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.utils import to_undirected\n",
        "from torch_geometric.nn import GCNConv, GATConv, JumpingKnowledge,SGConv\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm"
      ],
      "metadata": {
        "id": "6m5QESo6if9j"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define some functions and classes\n",
        "# Reference: https://github.com/CUAI/Non-Homophily-Benchmarks"
      ],
      "metadata": {
        "id": "9ucd-wqs0wwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Autoencoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, 32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(32, output_size),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size,output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(input_size, 32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(32, output_size),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.decoder(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "xir34NCwa9y3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Functions copied from https://github.com/CUAI/Non-Homophily-Benchmarks\n",
        "class NCDataset(object):\n",
        "    def __init__(self, name, root=f'{DATAPATH}'):\n",
        "        self.name = name  # original name, e.g., ogbn-proteins\n",
        "        self.graph = {}\n",
        "        self.label = None\n",
        "        self.original_label= None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert idx == 0, 'This dataset has only one graph'\n",
        "        return self.graph, self.label\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "    def __repr__(self):  \n",
        "        return '{}({})'.format(self.__class__.__name__, len(self))\n",
        "    \n",
        "def load_amazon_dataset():\n",
        "    fulldata = scipy.io.loadmat(DATAPATH)\n",
        "    A = fulldata['homo']\n",
        "    edge_index = np.array(A.nonzero())\n",
        "    node_feat = fulldata['features']\n",
        "    label = np.array(fulldata['label'], dtype=np.int).flatten()\n",
        "    num_nodes = node_feat.shape[0]\n",
        "\n",
        "    dataset = NCDataset('Amazon')\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
        "    node_feat = torch.tensor(node_feat.todense(), dtype=torch.float)\n",
        "    dataset.graph = {'edge_index': edge_index,\n",
        "                     'node_feat': node_feat,\n",
        "                     'edge_feat': None,\n",
        "                     'num_nodes': num_nodes}\n",
        "    label = torch.tensor(label, dtype=torch.long)\n",
        "    dataset.label = label\n",
        "    dataset.original_label = label\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def normalize(edge_index):\n",
        "    \"\"\" normalizes the edge_index\n",
        "    \"\"\"\n",
        "    adj_t = edge_index.set_diag()\n",
        "    deg = adj_t.sum(dim=1).to(torch.float)\n",
        "    deg_inv_sqrt = deg.pow(-0.5)\n",
        "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "    adj_t = deg_inv_sqrt.view(-1, 1) * adj_t * deg_inv_sqrt.view(1, -1)\n",
        "    return adj_t\n",
        "\n",
        "def eval_acc(y_true, y_pred):\n",
        "    acc_list = []\n",
        "    y_true = y_true.detach().cpu().numpy()\n",
        "    y_pred = y_pred.argmax(dim=-1, keepdim=True).detach().cpu().numpy()\n",
        "\n",
        "    for i in range(y_true.shape[1]):\n",
        "        is_labeled = y_true[:, i] == y_true[:, i]\n",
        "        correct = y_true[is_labeled, i] == y_pred[is_labeled, i]\n",
        "        acc_list.append(float(np.sum(correct))/len(correct))\n",
        "\n",
        "    return sum(acc_list)/len(acc_list)\n",
        "\n",
        "\n",
        "def eval_rocauc(y_true, y_pred):\n",
        "    \"\"\" adapted from ogb\n",
        "    https://github.com/snap-stanford/ogb/blob/master/ogb/nodeproppred/evaluate.py\"\"\"\n",
        "    rocauc_list = []\n",
        "    y_true = y_true.detach().cpu().numpy()\n",
        "    if y_true.shape[1] == 1:\n",
        "        # use the predicted class for single-class classification\n",
        "        y_pred = F.softmax(y_pred, dim=-1)[:,1].unsqueeze(1).detach().cpu().numpy()\n",
        "    else:\n",
        "        y_pred = y_pred.detach().cpu().numpy()\n",
        "\n",
        "    for i in range(y_true.shape[1]):\n",
        "        # AUC is only defined when there is at least one positive data.\n",
        "        if np.sum(y_true[:, i] == 1) > 0 and np.sum(y_true[:, i] == 0) > 0:\n",
        "            is_labeled = y_true[:, i] == y_true[:, i]\n",
        "            score = roc_auc_score(y_true[is_labeled, i], y_pred[is_labeled, i])\n",
        "                                \n",
        "            rocauc_list.append(score)\n",
        "\n",
        "    if len(rocauc_list) == 0:\n",
        "        print('No positively labeled data available. Cannot compute ROC-AUC.')\n",
        "        return 0\n",
        "    return sum(rocauc_list)/len(rocauc_list)\n",
        "\n",
        "\n",
        "def evaluate(model, dataset, split_idx, eval_func, result=None):\n",
        "    if result is not None:\n",
        "        out = result\n",
        "    else:\n",
        "        model.eval()\n",
        "        out = model(dataset)\n",
        "\n",
        "    train_acc = eval_func(\n",
        "        dataset.label[split_idx['train']], out[split_idx['train']])\n",
        "    test_acc = eval_func(\n",
        "        dataset.label[split_idx['test']], out[split_idx['test']])\n",
        "\n",
        "    return train_acc, test_acc, out\n",
        "\n",
        "#Models\n",
        "class LINK(nn.Module):\n",
        "    \"\"\" logistic regression on adjacency matrix \"\"\"\n",
        "    \n",
        "    def __init__(self, num_nodes, out_channels):\n",
        "        super(LINK, self).__init__()\n",
        "        self.W = nn.Linear(num_nodes, out_channels)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.W.reset_parameters()\n",
        "        \n",
        "    def forward(self, data):\n",
        "        N = data.graph['num_nodes']\n",
        "        edge_index = data.graph['edge_index']\n",
        "        if isinstance(edge_index, torch.Tensor):\n",
        "            row, col = edge_index\n",
        "            A = SparseTensor(row=row, col=col, sparse_sizes=(N, N)).to_torch_sparse_coo_tensor()\n",
        "        elif isinstance(edge_index, SparseTensor):\n",
        "            A = edge_index.to_torch_sparse_coo_tensor()\n",
        "        logits = self.W(A)\n",
        "        return logits   \n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2,\n",
        "                 dropout=0.5, heads=2):\n",
        "        super(GAT, self).__init__()\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(\n",
        "            GATConv(in_channels, hidden_channels, heads=heads, concat=True))\n",
        "\n",
        "        self.bns = nn.ModuleList()\n",
        "        self.bns.append(nn.BatchNorm1d(hidden_channels*heads))\n",
        "        for _ in range(num_layers - 2):\n",
        "\n",
        "            self.convs.append(\n",
        "                    GATConv(hidden_channels*heads, hidden_channels, heads=heads, concat=True) ) \n",
        "            self.bns.append(nn.BatchNorm1d(hidden_channels*heads))\n",
        "\n",
        "        self.convs.append(\n",
        "            GATConv(hidden_channels*heads, out_channels, heads=heads, concat=False))\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.activation = F.elu \n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data.graph['node_feat']\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, data.graph['edge_index'])\n",
        "            x = self.bns[i](x)\n",
        "            x = self.activation(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, data.graph['edge_index'])\n",
        "        return x\n",
        "    \n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2,\n",
        "                 dropout=0.5, save_mem=False, use_bn=True):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(\n",
        "            GCNConv(in_channels, hidden_channels, cached=not save_mem, normalize=not save_mem))\n",
        "\n",
        "        self.bns = nn.ModuleList()\n",
        "        self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(\n",
        "                GCNConv(hidden_channels, hidden_channels, cached=not save_mem, normalize=not save_mem))\n",
        "            self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
        "\n",
        "        self.convs.append(\n",
        "            GCNConv(hidden_channels, out_channels, cached=not save_mem, normalize=not save_mem))\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.activation = F.relu\n",
        "        self.use_bn = use_bn\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data.graph['node_feat']\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, data.graph['edge_index'])\n",
        "            if self.use_bn:\n",
        "                x = self.bns[i](x)\n",
        "            x = self.activation(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, data.graph['edge_index'])\n",
        "        return x\n",
        "class SGC(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, hops):\n",
        "        \"\"\" takes 'hops' power of the normalized adjacency\"\"\"\n",
        "        super(SGC, self).__init__()\n",
        "        self.conv = SGConv(in_channels, out_channels, hops, cached=True) \n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv.reset_parameters()\n",
        "\n",
        "    def forward(self, data):\n",
        "        edge_index = data.graph['edge_index']\n",
        "        x = data.graph['node_feat']\n",
        "        x = self.conv(x, edge_index)\n",
        "        return x\n",
        "\n",
        "class APPNP_Net(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=.5, K=10, alpha=.1):\n",
        "        super(APPNP_Net, self).__init__()\n",
        "        self.lin1 = nn.Linear(in_channels, hidden_channels)\n",
        "        self.lin2 = nn.Linear(hidden_channels, out_channels)\n",
        "        self.prop1 = APPNP(K, alpha)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.lin1.reset_parameters()\n",
        "        self.lin2.reset_parameters()\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.graph['node_feat'], data.graph['edge_index']\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.lin2(x)\n",
        "        x = self.prop1(x, edge_index)\n",
        "        return x\n",
        "        \n",
        "class GCNJK(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2,\n",
        "                 dropout=0.5, save_mem=False, jk_type='max'):\n",
        "        super(GCNJK, self).__init__()\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(\n",
        "            GCNConv(in_channels, hidden_channels, cached=not save_mem, normalize=not save_mem))\n",
        "\n",
        "        self.bns = nn.ModuleList()\n",
        "        self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(\n",
        "                GCNConv(hidden_channels, hidden_channels, cached=not save_mem, normalize=not save_mem))\n",
        "            self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
        "\n",
        "        self.convs.append(\n",
        "            GCNConv(hidden_channels, hidden_channels, cached=not save_mem, normalize=not save_mem))\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.activation = F.relu\n",
        "        self.jump = JumpingKnowledge(jk_type, channels=hidden_channels, num_layers=1)\n",
        "        if jk_type == 'cat':\n",
        "            self.final_project = nn.Linear(hidden_channels * num_layers, out_channels)\n",
        "        else: # max or lstm\n",
        "            self.final_project = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "        self.jump.reset_parameters()\n",
        "        self.final_project.reset_parameters()\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data.graph['node_feat']\n",
        "        xs = []\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, data.graph['edge_index'])\n",
        "            x = self.bns[i](x)\n",
        "            x = self.activation(x)\n",
        "            xs.append(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, data.graph['edge_index'])\n",
        "        xs.append(x)\n",
        "        x = self.jump(xs)\n",
        "        x = self.final_project(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "9-7JCwt7i5t-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some hyper parameters"
      ],
      "metadata": {
        "id": "5ie9rPZNGg9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training set and test set portion size\n",
        "train_portion=0.4\n",
        "test_portion=0.6\n",
        "\n",
        "#Train with GAT will require 40GB GPU memory (Colab Pro+)\n",
        "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "#device= 'cpu'\n",
        "device = torch.device(device)\n",
        "\n",
        "#Parameters for GNN models\n",
        "hidden_channels=32\n",
        "num_layers=2\n",
        "dropout=0\n",
        "lr=0.01\n",
        "weight_decay=1e-4\n",
        "batch_size=512 "
      ],
      "metadata": {
        "id": "3O3w3Z6FjJAl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a data structure and store the graph in it\n",
        "### --Used for training GNNs"
      ],
      "metadata": {
        "id": "4nSCBQk2FzYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=load_amazon_dataset()\n",
        "if len(dataset.label.shape) == 1:\n",
        "    dataset.label = dataset.label.unsqueeze(1)\n",
        "    dataset.original_label= dataset.original_label.unsqueeze(1)\n",
        "\n",
        "dataset.label = dataset.label.to(device)\n",
        "dataset.original_label= dataset.original_label.to(device)\n",
        "n = dataset.graph['num_nodes']\n",
        "# infer the number of classes for non one-hot and one-hot labels\n",
        "c = max(dataset.label.max().item() + 1, dataset.label.shape[1])\n",
        "d = dataset.graph['node_feat'].shape[1]\n",
        "dataset.graph['edge_index'] = to_undirected(dataset.graph['edge_index'])\n",
        "dataset.graph['edge_index'], dataset.graph['node_feat'] = \\\n",
        "    dataset.graph['edge_index'].to(device), dataset.graph['node_feat'].to(device)\n",
        "print(f\"num nodes {n} | num classes {c} | num node feats {d}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoJQDmrikfDn",
        "outputId": "977d1a84-f989-4143-fbbe-e44c762d5bcf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-286b88176b1d>:24: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  label = np.array(fulldata['label'], dtype=np.int).flatten()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num nodes 11944 | num classes 2 | num node feats 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the feature matrix for training Autoencoders"
      ],
      "metadata": {
        "id": "BRfHF3zuGA27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amazon = loadmat(DATAPATH)\n",
        "\n",
        "#adj = amazon['homo'].tocsr() # Adjacency matrix\n",
        "features=amazon['features'].toarray() # Feature matrix\n",
        "label_origin=amazon['label'][0]\n",
        "\n",
        "n_nodes, feat_dim = features.shape\n",
        "\n",
        "#Split train and test set\n",
        "all_index=np.arange(n_nodes)\n",
        "X_train, X_test, y_train, y_test=train_test_split(all_index, label_origin, test_size=test_portion, stratify=label_origin, random_state=66)\n",
        "original_index={'X_train':X_train, 'X_test':X_test, 'y_train':y_train, 'y_test':y_test}"
      ],
      "metadata": {
        "id": "VYNh3Qh4bA2V"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Autoencoder"
      ],
      "metadata": {
        "id": "cR6zdIsh6af5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Split a validation set for performance evaluation\n",
        "x_train, x_valid, y1, y2=train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=66)\n",
        "code_size=20\n",
        "criterion_decoder = nn.MSELoss()\n",
        "\n",
        "#Get index for fraud and non-fraud nodes in training set\n",
        "idx_NF=x_train[y1==0]\n",
        "idx_F=x_train[y1==1]\n",
        "\n",
        "features_NF=features[idx_NF]\n",
        "features_F=features[idx_F]\n",
        "features_test=features[X_test]"
      ],
      "metadata": {
        "id": "7Ebkz56xw217"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#L1=Loss of Autoencoder trained on non-fraud samples; L2=Loss of Autoencoder trained on fraud samples\n",
        "NF_ratio=0 # The ratio of non-fraud nodes that L1<L2.\n",
        "F_ratio=0 # The ratio of fraud nodes that L1>L2.\n",
        "encoder_NF=None\n",
        "decoder_NF=None\n",
        "encoder_F=None\n",
        "decoder_F=None\n",
        "#Train until both ratio are larger than 0.8\n",
        "while(NF_ratio<0.8 or F_ratio<0.8):\n",
        "  #Autoencoder for non-fraud nodes\n",
        "  encoder_NF = Encoder(input_size=25,output_size=code_size)\n",
        "  decoder_NF = Decoder(input_size=code_size,output_size=25)\n",
        "  optimizer1 = optim.Adam(list(encoder_NF.parameters()) + list(decoder_NF.parameters()),\n",
        "                        lr=0.001, weight_decay=1e-4)\n",
        "  for epoch in range(100):\n",
        "      optimizer1.zero_grad()\n",
        "      # Forward\n",
        "      encoder_out = encoder_NF(torch.from_numpy(features_NF).float())\n",
        "      decoder_out = decoder_NF(encoder_out)\n",
        "      # Loss\n",
        "      train_loss_decoder = criterion_decoder(decoder_out, torch.from_numpy(features_NF).float())\n",
        "      #Backpropagation\n",
        "      train_loss_decoder.backward()\n",
        "      optimizer1.step()    \n",
        "\n",
        "  #Autoencoder for fraud nodes\n",
        "  encoder_F = Encoder(input_size=25,output_size=code_size)\n",
        "  decoder_F = Decoder(input_size=code_size,output_size=25)\n",
        "  optimizer2 = optim.Adam(list(encoder_F.parameters()) + list(decoder_F.parameters()),\n",
        "                        lr=0.001, weight_decay=1e-4)\n",
        "  for epoch in range(200):\n",
        "      optimizer2.zero_grad()\n",
        "      # Forward\n",
        "      encoder_out = encoder_F(torch.from_numpy(features_F).float())\n",
        "      decoder_out = decoder_F(encoder_out)\n",
        "      # Loss\n",
        "      train_loss_decoder = criterion_decoder(decoder_out, torch.from_numpy(features_F).float())\n",
        "      #Backpropagation\n",
        "      train_loss_decoder.backward()\n",
        "      optimizer2.step()\n",
        "\n",
        "  #Check performance\n",
        "  NF_F=[]\n",
        "  NF_NF=[]\n",
        "  for i in x_valid[y2==0]:\n",
        "    NF_valid_tensor=torch.from_numpy(features[i]).float()\n",
        "    encoder_out = encoder_F.forward(NF_valid_tensor)\n",
        "    decoder_out = decoder_F.forward(encoder_out)\n",
        "    loss=criterion_decoder(decoder_out, NF_valid_tensor).item()\n",
        "    NF_F.append(round(loss,6))\n",
        "\n",
        "    NF_valid_tensor=torch.from_numpy(features[i]).float()\n",
        "    encoder_out = encoder_NF.forward(NF_valid_tensor)\n",
        "    decoder_out = decoder_NF.forward(encoder_out)\n",
        "    loss=criterion_decoder(decoder_out, NF_valid_tensor).item()\n",
        "    NF_NF.append(round(loss,6))\n",
        "\n",
        "  F_NF=[]\n",
        "  F_F=[]\n",
        "  for i in x_valid[y2==1]:\n",
        "    F_valid_tensor=torch.from_numpy(features[i]).float()\n",
        "    encoder_out = encoder_NF.forward(F_valid_tensor)\n",
        "    decoder_out = decoder_NF.forward(encoder_out)\n",
        "    loss=criterion_decoder(decoder_out, F_valid_tensor).item()\n",
        "    F_NF.append(round(loss,6))\n",
        "\n",
        "    F_valid_tensor=torch.from_numpy(features[i]).float()\n",
        "    encoder_out = encoder_F.forward(F_valid_tensor)\n",
        "    decoder_out = decoder_F.forward(encoder_out)\n",
        "    loss=criterion_decoder(decoder_out, F_valid_tensor).item()\n",
        "    F_F.append(round(loss,6))\n",
        "\n",
        "  NF_F=np.array(NF_F)  \n",
        "  NF_NF=np.array(NF_NF)        \n",
        "  F_F=np.array(F_F)  \n",
        "  F_NF=np.array(F_NF)      \n",
        "\n",
        "  NF_ratio=((NF_NF<NF_F).sum())/len(NF_F)\n",
        "  F_ratio=((F_F<F_NF).sum())/len(F_F)"
      ],
      "metadata": {
        "id": "1QYjpicWcEuS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'NF_ratio: {NF_ratio}, F_ratio: {F_ratio}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdtdAhtey3l_",
        "outputId": "3c479758-082e-4b8f-d70e-222b1e968ffa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NF_ratio: 0.8696629213483146, F_ratio: 0.8333333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train GNN models"
      ],
      "metadata": {
        "id": "a_U2C6JCk-vR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define loss function and evaluation metric\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "eval_func = eval_rocauc # eval_acc: Accuracy "
      ],
      "metadata": {
        "id": "Fs63VLHOP-nr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keep_training=1\n",
        "iteration=1\n",
        "no_improvement_round=0\n",
        "code_size=16 # Code size of Autoencoder\n",
        "probs=np.empty(shape=[0,2])#prediction probabilities\n",
        "pseudo_labels=np.array([])\n",
        "while(keep_training):\n",
        "  start_time=time.time()\n",
        "  # If no new test nodes are labeled for 5 consecutive rounds\n",
        "  if no_improvement_round==1:\n",
        "    break\n",
        "\n",
        "  print(f'********* Iteration {iteration} *********\\n')\n",
        "\n",
        "  ########Calculate Autoencoder losss L1 and L2, and Hint########\n",
        "  hint=[]\n",
        "  abs_diff=[]\n",
        "  for i in X_test:\n",
        "    sample=torch.from_numpy(features[i]).float()\n",
        "\n",
        "    encoder_out = encoder_F(sample)\n",
        "    decoder_out = decoder_F(encoder_out)\n",
        "    loss1=criterion_decoder(decoder_out,sample).item()\n",
        "\n",
        "    encoder_out2 = encoder_NF(sample)\n",
        "    decoder_out2 = decoder_NF(encoder_out2)\n",
        "    loss2=criterion_decoder(decoder_out2,sample).item()\n",
        "\n",
        "    abs_diff.append(abs(loss1-loss2))\n",
        "    hint.append(0 if loss1>loss2 else 1)\n",
        "  hint=np.array(hint)\n",
        "  abs_diff=np.array(abs_diff)\n",
        "\n",
        "  ######## Update the indexes for taning GNNs ########\n",
        "  split_idx = {}# Define an empty dictionary to store training and test index list\n",
        "  split_idx['train']=torch.tensor(np.array(X_train)).to(device)\n",
        "  split_idx['test']=torch.tensor(np.array(X_test)).to(device)\n",
        "\n",
        "  train_idx = split_idx['train']\n",
        "  #model2 = MixHop(d, hidden_channels, c, num_layers=num_layers,\n",
        "                       #dropout=dropout, hops=1).to(device)\n",
        "  #model1 = GCNJK(d, hidden_channels, c, num_layers=num_layers, dropout=dropout, jk_type='max').to(device)\n",
        "  model1 = GCN(in_channels=d,hidden_channels=hidden_channels,out_channels=c,num_layers=num_layers,dropout=dropout,use_bn=False).to(device)\n",
        "  optimizer1 = torch.optim.AdamW(model1.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  #model2 = GAT(d, hidden_channels, c, num_layers=num_layers, dropout=dropout, heads=gat_heads).to(device)\n",
        "  model2 = SGC(in_channels=d, out_channels=c, hops=1).to(device)\n",
        "  #model2 = GCN(in_channels=d,hidden_channels=hidden_channels,out_channels=c,num_layers=num_layers,dropout=dropout,use_bn=False).to(device)\n",
        "  optimizer2 = torch.optim.AdamW(model2.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  model3 = LINK(n, c).to(device)\n",
        "  optimizer3 = torch.optim.AdamW(model3.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  model4 = GCNJK(d, hidden_channels, c, num_layers=num_layers, dropout=dropout, jk_type='max').to(device)\n",
        "  optimizer4 = torch.optim.AdamW(model4.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  model1.reset_parameters()\n",
        "  model2.reset_parameters()\n",
        "  model3.reset_parameters()\n",
        "  model4.reset_parameters()\n",
        "  print(f'GNN training nodes: {len(train_idx)}')\n",
        "  ########Train the first model########\n",
        "  for epoch in range(50):       \n",
        "    num_batches = int(len(train_idx) / batch_size) + 1\n",
        "    epoch_loss = 0.0    \n",
        "    for batch in range(num_batches):\n",
        "      i_start = batch * batch_size\n",
        "      i_end = min((batch + 1) * batch_size, len(train_idx))\n",
        "      batch_nodes = train_idx[i_start:i_end]\n",
        "              \n",
        "      model1.train()\n",
        "      #torch.cuda.empty_cache()\n",
        "      optimizer1.zero_grad()\n",
        "      #torch.cuda.empty_cache()\n",
        "      out = model1(dataset)\n",
        "      #torch.cuda.empty_cache()\n",
        "      if dataset.label.shape[1] == 1:\n",
        "          true_label = F.one_hot(dataset.label, dataset.label.max() + 1).squeeze(1)\n",
        "      else:\n",
        "          true_label = dataset.label\n",
        "      #torch.cuda.empty_cache()\n",
        "      loss = criterion(out[batch_nodes], true_label.squeeze(1)[batch_nodes].to(torch.float))\n",
        "      #torch.cuda.empty_cache()\n",
        "      loss.backward()\n",
        "      #torch.cuda.empty_cache()\n",
        "      optimizer1.step()\n",
        "      #torch.cuda.empty_cache()\n",
        "      epoch_loss += loss.item() \n",
        "      '''del loss,out,batch_nodes,true_label\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.empty_cache()   \n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.empty_cache() '''    \n",
        "    \n",
        "    if (epoch+1)%10==0:\n",
        "      result = evaluate(model1, dataset, split_idx, eval_func)\n",
        "      torch.cuda.empty_cache()\n",
        "      print(f'Model 1, '\n",
        "        f'Epoch: {epoch+1:02d}, '\n",
        "        f'Loss: {epoch_loss / num_batches:.4f}, '\n",
        "        f'Train AUC: {100 * result[0]:.2f}%, '\n",
        "        f'Test AUC: {100 * result[1]:.2f}%')\n",
        "      '''del result\n",
        "      torch.cuda.empty_cache()   \n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.empty_cache()\n",
        "    torch.cuda.empty_cache()   \n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.empty_cache()'''\n",
        "  ########Train the second model########\n",
        "  for epoch in range(50):\n",
        "    num_batches = int(len(train_idx) / batch_size) + 1\n",
        "    epoch_loss = 0.0     \n",
        "    for batch in range(num_batches):\n",
        "      i_start = batch * batch_size\n",
        "      i_end = min((batch + 1) * batch_size, len(train_idx))\n",
        "      batch_nodes = train_idx[i_start:i_end]\n",
        "                \n",
        "      model2.train()\n",
        "      #torch.cuda.empty_cache()\n",
        "      optimizer2.zero_grad()\n",
        "      #torch.cuda.empty_cache()\n",
        "      out = model2(dataset)\n",
        "      #torch.cuda.empty_cache()\n",
        "      if dataset.label.shape[1] == 1:\n",
        "          true_label = F.one_hot(dataset.label, dataset.label.max() + 1).squeeze(1)\n",
        "      else:\n",
        "          true_label = dataset.label\n",
        "      #torch.cuda.empty_cache()\n",
        "      loss = criterion(out[batch_nodes], true_label.squeeze(1)[batch_nodes].to(torch.float))\n",
        "      #torch.cuda.empty_cache()\n",
        "      #torch.cuda.empty_cache()\n",
        "      loss.backward()\n",
        "      #torch.cuda.empty_cache()\n",
        "      #torch.cuda.empty_cache()\n",
        "      epoch_loss += loss.item()\n",
        "      #torch.cuda.empty_cache()\n",
        "      #torch.cuda.empty_cache()\n",
        "      optimizer2.step()\n",
        "      '''del loss, out, batch_nodes, true_label\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.empty_cache()          \n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.empty_cache()'''\n",
        "    \n",
        "    if (epoch+1)%10==0:\n",
        "      result = evaluate(model2, dataset, split_idx, eval_func)\n",
        "      torch.cuda.empty_cache()\n",
        "      print(f'Model 2, '\n",
        "        f'Epoch: {epoch+1:02d}, '\n",
        "        f'Loss: {epoch_loss / num_batches:.4f}, '\n",
        "        f'Train AUC: {100 * result[0]:.2f}%, '\n",
        "        f'Test AUC: {100 * result[1]:.2f}%')\n",
        "      '''del result\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.empty_cache()'''\n",
        "\n",
        "  ########Train the third model########\n",
        "  for epoch in range(50):\n",
        "    num_batches = int(len(train_idx) / batch_size) + 1\n",
        "    epoch_loss = 0.0     \n",
        "    for batch in range(num_batches):\n",
        "      i_start = batch * batch_size\n",
        "      i_end = min((batch + 1) * batch_size, len(train_idx))\n",
        "      batch_nodes = train_idx[i_start:i_end]\n",
        "                \n",
        "      model3.train()\n",
        "      #torch.cuda.empty_cache()\n",
        "      optimizer3.zero_grad()\n",
        "      #torch.cuda.empty_cache()\n",
        "      out = model3(dataset)\n",
        "      #torch.cuda.empty_cache()\n",
        "      if dataset.label.shape[1] == 1:\n",
        "          true_label = F.one_hot(dataset.label, dataset.label.max() + 1).squeeze(1)\n",
        "      else:\n",
        "          true_label = dataset.label\n",
        "      #torch.cuda.empty_cache()\n",
        "      loss = criterion(out[batch_nodes], true_label.squeeze(1)[batch_nodes].to(torch.float))\n",
        "      #torch.cuda.empty_cache()\n",
        "      #torch.cuda.empty_cache()\n",
        "      loss.backward()\n",
        "      #torch.cuda.empty_cache()\n",
        "      #torch.cuda.empty_cache()\n",
        "      epoch_loss += loss.item()\n",
        "      #torch.cuda.empty_cache()\n",
        "      #torch.cuda.empty_cache()\n",
        "      optimizer3.step()\n",
        "      del loss, out, batch_nodes, true_label\n",
        "      torch.cuda.empty_cache()         \n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    \n",
        "    if (epoch+1)%10==0:\n",
        "      result = evaluate(model3, dataset, split_idx, eval_func)\n",
        "      torch.cuda.empty_cache()\n",
        "      print(f'Model 3, '\n",
        "        f'Epoch: {epoch+1:02d}, '\n",
        "        f'Loss: {epoch_loss / num_batches:.4f}, '\n",
        "        f'Train AUC: {100 * result[0]:.2f}%, '\n",
        "        f'Test AUC: {100 * result[1]:.2f}%')\n",
        "      '''del result\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.empty_cache()'''\n",
        "  ########Train the third model########\n",
        "  for epoch in range(50):\n",
        "    num_batches = int(len(train_idx) / batch_size) + 1\n",
        "    epoch_loss = 0.0     \n",
        "    for batch in range(num_batches):\n",
        "      i_start = batch * batch_size\n",
        "      i_end = min((batch + 1) * batch_size, len(train_idx))\n",
        "      batch_nodes = train_idx[i_start:i_end]\n",
        "                \n",
        "      model4.train()\n",
        "      #torch.cuda.empty_cache()\n",
        "      optimizer4.zero_grad()\n",
        "      #torch.cuda.empty_cache()\n",
        "      out = model4(dataset)\n",
        "      #torch.cuda.empty_cache()\n",
        "      if dataset.label.shape[1] == 1:\n",
        "          true_label = F.one_hot(dataset.label, dataset.label.max() + 1).squeeze(1)\n",
        "      else:\n",
        "          true_label = dataset.label\n",
        "      #torch.cuda.empty_cache()\n",
        "      loss = criterion(out[batch_nodes], true_label.squeeze(1)[batch_nodes].to(torch.float))\n",
        "      #torch.cuda.empty_cache()\n",
        "      #torch.cuda.empty_cache()\n",
        "      loss.backward()\n",
        "      #torch.cuda.empty_cache()\n",
        "      #torch.cuda.empty_cache()\n",
        "      epoch_loss += loss.item()\n",
        "      #torch.cuda.empty_cache()\n",
        "      #torch.cuda.empty_cache()\n",
        "      optimizer4.step()\n",
        "      del loss, out, batch_nodes, true_label\n",
        "      torch.cuda.empty_cache()         \n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    \n",
        "    if (epoch+1)%10==0:\n",
        "      result = evaluate(model4, dataset, split_idx, eval_func)\n",
        "      torch.cuda.empty_cache()\n",
        "      print(f'Model 4, '\n",
        "        f'Epoch: {epoch+1:02d}, '\n",
        "        f'Loss: {epoch_loss / num_batches:.4f}, '\n",
        "        f'Train AUC: {100 * result[0]:.2f}%, '\n",
        "        f'Test AUC: {100 * result[1]:.2f}%')\n",
        "      '''del result\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.empty_cache()'''\n",
        "\n",
        "  ########Get probabilities########\n",
        "  result1 = evaluate(model1, dataset, split_idx, eval_func)\n",
        "  torch.cuda.empty_cache()\n",
        "  AUC1=result1[0]\n",
        "  prob1=result1[2][split_idx['test']]\n",
        "  prob1=F.softmax(prob1, dim=-1).detach().cpu().numpy()\n",
        "\n",
        "  result2 = evaluate(model2, dataset, split_idx, eval_func)\n",
        "  torch.cuda.empty_cache()\n",
        "  AUC2=result2[0]\n",
        "  prob2=result2[2][split_idx['test']]\n",
        "  prob2=F.softmax(prob2, dim=-1).detach().cpu().numpy()\n",
        "\n",
        "  result3 = evaluate(model3, dataset, split_idx, eval_func)\n",
        "  torch.cuda.empty_cache()\n",
        "  AUC3=result3[0]\n",
        "  prob3=result3[2][split_idx['test']]\n",
        "  prob3=F.softmax(prob3, dim=-1).detach().cpu().numpy()\n",
        "\n",
        "  result4 = evaluate(model4, dataset, split_idx, eval_func)\n",
        "  torch.cuda.empty_cache()\n",
        "  AUC4=result4[0]\n",
        "  prob4=result4[2][split_idx['test']]\n",
        "  prob4=F.softmax(prob4, dim=-1).detach().cpu().numpy()\n",
        "  \n",
        "  ########Ensemble probabilities########\n",
        "  prob_e=(prob1+prob2+prob3+prob4)/4 #Weighted average according to training AUC\n",
        "\n",
        "  ########Calculate entropy and confidence########\n",
        "  H=[] #Entropy\n",
        "  for i in prob_e:\n",
        "    h = entropy(i)\n",
        "    H.append(h)\n",
        "  Confidence=abs_diff+np.array(H) #Confidence score\n",
        "\n",
        "  ########Sort according to confidence and choose the first p nodes########\n",
        "  p=int(len(H)*0.5)\n",
        "\n",
        "  if p<=500:\n",
        "    p=len(H)\n",
        "\n",
        "  idx_sorted=np.array(sorted(range(len(Confidence)),key=Confidence.__getitem__,reverse=True))# Sort according to confidence score\n",
        "  idx_sorted=idx_sorted[0:p]# Choose first p samples\n",
        "\n",
        "  ########Assign pseudo labels########\n",
        "  pseudo_label=prob_e[idx_sorted].argmax(axis=1)\n",
        "\n",
        "  ########Only keep nodes that agree on both hint and pseudo label########\n",
        "  idx_agree=idx_sorted[pseudo_label==hint[idx_sorted]]# Index that agree on both hint and pseudo label\n",
        "\n",
        "  # If no new nodes to be labeled, skip this iteration\n",
        "  if len(idx_agree)<=10:\n",
        "    no_improvement_round+=1\n",
        "    iteration+=1\n",
        "    print('No new nodes can be labeled')\n",
        "    continue\n",
        "  else:\n",
        "    no_improvement_round=0\n",
        "\n",
        "  #Get pesudo labels that agree with hint\n",
        "  pseudo_label_agree=pseudo_label[pseudo_label==hint[idx_sorted]]\n",
        "\n",
        "  # Record the corresponding prediction probability for roc-auc calculation\n",
        "  probs=np.append(probs,prob_e[idx_agree],axis=0)\n",
        "\n",
        "  #Update labels\n",
        "  y_train=np.append(y_train, pseudo_label_agree)\n",
        "  pseudo_labels=np.append(pseudo_labels,pseudo_label_agree)\n",
        "  y_test=np.delete(y_test,idx_agree)\n",
        "  #Also update labels in \"dataset\" so that we can use the pseudo labels to train the model\n",
        "  for i in range(len(idx_agree)):\n",
        "    curr=X_test[idx_agree[i]]\n",
        "    dataset.label[curr]=pseudo_label_agree[i]\n",
        "\n",
        "  #Update indexes\n",
        "  X_train=np.append(X_train, X_test[idx_agree])#Update training index\n",
        "  X_test=np.delete(X_test, idx_agree)#Update test index\n",
        "\n",
        "\n",
        "  print(f'New labeled fraud nodes: {len(pseudo_label_agree[pseudo_label_agree==1])},'\n",
        "     f'New labeled non-fraud nodes: {len(pseudo_label_agree[pseudo_label_agree==0])}')\n",
        "  print(f'Total new labeled nodes: {len(idx_agree)}, remaining test nodes: {len(X_test)}')\n",
        "  iteration+=1\n",
        "  \n",
        "  # If all test nodes are lebeled, stop training\n",
        "  if len(X_test)==0:\n",
        "    keep_training=0\n",
        "\n",
        "  # Clear gpu memory  \n",
        "  del model1,model2,model3,optimizer1, optimizer2, optimizer3, result1, result2,result3,prob1,prob2,prob3,split_idx,train_idx\n",
        "  #del model1, model2, optimizer1, optimizer2\n",
        "  #Not sure how much is enough :D\n",
        "  torch.cuda.empty_cache()\n",
        "  torch.cuda.empty_cache()\n",
        "  torch.cuda.empty_cache()\n",
        "  torch.cuda.empty_cache()\n",
        "  torch.cuda.empty_cache()\n",
        "  torch.cuda.empty_cache()\n",
        "  torch.cuda.empty_cache()\n",
        "  torch.cuda.empty_cache()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "  end_time=time.time()\n",
        "  print(f'Time consumption: {int(end_time-start_time)} seconds')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zpjc5DteQqp7",
        "outputId": "da54c6f6-beee-42a1-abb9-35571ae1756f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* Iteration 1 *********\n",
            "\n",
            "GNN training nodes: 4777\n",
            "Model 1, Epoch: 10, Loss: 0.2222, Train AUC: 79.33%, Test AUC: 79.11%\n",
            "Model 1, Epoch: 20, Loss: 0.2185, Train AUC: 79.74%, Test AUC: 79.49%\n",
            "Model 1, Epoch: 30, Loss: 0.2193, Train AUC: 80.17%, Test AUC: 79.88%\n",
            "Model 1, Epoch: 40, Loss: 0.2295, Train AUC: 80.41%, Test AUC: 80.13%\n",
            "Model 1, Epoch: 50, Loss: 0.2313, Train AUC: 80.66%, Test AUC: 80.36%\n",
            "Model 2, Epoch: 10, Loss: 0.2091, Train AUC: 82.31%, Test AUC: 82.45%\n",
            "Model 2, Epoch: 20, Loss: 0.1966, Train AUC: 83.63%, Test AUC: 83.53%\n",
            "Model 2, Epoch: 30, Loss: 0.1936, Train AUC: 84.15%, Test AUC: 84.05%\n",
            "Model 2, Epoch: 40, Loss: 0.1916, Train AUC: 84.48%, Test AUC: 84.38%\n",
            "Model 2, Epoch: 50, Loss: 0.1900, Train AUC: 84.73%, Test AUC: 84.63%\n",
            "Model 3, Epoch: 10, Loss: 0.0504, Train AUC: 99.97%, Test AUC: 84.15%\n",
            "Model 3, Epoch: 20, Loss: 0.0218, Train AUC: 100.00%, Test AUC: 84.29%\n",
            "Model 3, Epoch: 30, Loss: 0.0126, Train AUC: 100.00%, Test AUC: 84.32%\n",
            "Model 3, Epoch: 40, Loss: 0.0083, Train AUC: 100.00%, Test AUC: 84.31%\n",
            "Model 3, Epoch: 50, Loss: 0.0059, Train AUC: 100.00%, Test AUC: 84.31%\n",
            "Model 4, Epoch: 10, Loss: 0.1973, Train AUC: 83.67%, Test AUC: 83.25%\n",
            "Model 4, Epoch: 20, Loss: 0.1725, Train AUC: 87.32%, Test AUC: 86.20%\n",
            "Model 4, Epoch: 30, Loss: 0.1666, Train AUC: 88.40%, Test AUC: 86.84%\n",
            "Model 4, Epoch: 40, Loss: 0.1628, Train AUC: 89.65%, Test AUC: 87.65%\n",
            "Model 4, Epoch: 50, Loss: 0.1616, Train AUC: 89.61%, Test AUC: 87.75%\n",
            "New labeled fraud nodes: 53,New labeled non-fraud nodes: 2771\n",
            "Total new labeled nodes: 2824, remaining test nodes: 4343\n",
            "Time consumption: 149 seconds\n",
            "********* Iteration 2 *********\n",
            "\n",
            "GNN training nodes: 7601\n",
            "Model 1, Epoch: 10, Loss: 0.2481, Train AUC: 82.44%, Test AUC: 77.99%\n",
            "Model 1, Epoch: 20, Loss: 0.2421, Train AUC: 82.53%, Test AUC: 78.05%\n",
            "Model 1, Epoch: 30, Loss: 0.2205, Train AUC: 82.62%, Test AUC: 78.13%\n",
            "Model 1, Epoch: 40, Loss: 0.1988, Train AUC: 82.68%, Test AUC: 78.20%\n",
            "Model 1, Epoch: 50, Loss: 0.1933, Train AUC: 82.98%, Test AUC: 78.47%\n",
            "Model 2, Epoch: 10, Loss: 0.2130, Train AUC: 84.45%, Test AUC: 80.82%\n",
            "Model 2, Epoch: 20, Loss: 0.2016, Train AUC: 85.54%, Test AUC: 81.71%\n",
            "Model 2, Epoch: 30, Loss: 0.1970, Train AUC: 86.00%, Test AUC: 82.12%\n",
            "Model 2, Epoch: 40, Loss: 0.1946, Train AUC: 86.27%, Test AUC: 82.34%\n",
            "Model 2, Epoch: 50, Loss: 0.1932, Train AUC: 86.44%, Test AUC: 82.50%\n",
            "Model 3, Epoch: 10, Loss: 0.0358, Train AUC: 99.98%, Test AUC: 83.73%\n",
            "Model 3, Epoch: 20, Loss: 0.0145, Train AUC: 100.00%, Test AUC: 83.98%\n",
            "Model 3, Epoch: 30, Loss: 0.0082, Train AUC: 100.00%, Test AUC: 84.01%\n",
            "Model 3, Epoch: 40, Loss: 0.0053, Train AUC: 100.00%, Test AUC: 84.01%\n",
            "Model 3, Epoch: 50, Loss: 0.0038, Train AUC: 100.00%, Test AUC: 84.00%\n",
            "Model 4, Epoch: 10, Loss: 0.1661, Train AUC: 83.40%, Test AUC: 78.44%\n",
            "Model 4, Epoch: 20, Loss: 0.1450, Train AUC: 86.30%, Test AUC: 80.95%\n",
            "Model 4, Epoch: 30, Loss: 0.1454, Train AUC: 87.35%, Test AUC: 82.35%\n",
            "Model 4, Epoch: 40, Loss: 0.1351, Train AUC: 88.46%, Test AUC: 83.67%\n",
            "Model 4, Epoch: 50, Loss: 0.1333, Train AUC: 88.98%, Test AUC: 84.06%\n",
            "New labeled fraud nodes: 4,New labeled non-fraud nodes: 1318\n",
            "Total new labeled nodes: 1322, remaining test nodes: 3021\n",
            "Time consumption: 216 seconds\n",
            "********* Iteration 3 *********\n",
            "\n",
            "GNN training nodes: 8923\n",
            "Model 1, Epoch: 10, Loss: 0.6304, Train AUC: 82.24%, Test AUC: 78.39%\n",
            "Model 1, Epoch: 20, Loss: 0.2230, Train AUC: 82.39%, Test AUC: 78.77%\n",
            "Model 1, Epoch: 30, Loss: 0.1983, Train AUC: 82.45%, Test AUC: 78.82%\n",
            "Model 1, Epoch: 40, Loss: 0.1801, Train AUC: 82.61%, Test AUC: 78.91%\n",
            "Model 1, Epoch: 50, Loss: 0.1686, Train AUC: 82.88%, Test AUC: 79.17%\n",
            "Model 2, Epoch: 10, Loss: 0.2248, Train AUC: 84.35%, Test AUC: 81.58%\n",
            "Model 2, Epoch: 20, Loss: 0.2117, Train AUC: 85.12%, Test AUC: 82.10%\n",
            "Model 2, Epoch: 30, Loss: 0.2068, Train AUC: 85.49%, Test AUC: 82.36%\n",
            "Model 2, Epoch: 40, Loss: 0.2043, Train AUC: 85.71%, Test AUC: 82.53%\n",
            "Model 2, Epoch: 50, Loss: 0.2029, Train AUC: 85.86%, Test AUC: 82.65%\n",
            "Model 3, Epoch: 10, Loss: 0.0295, Train AUC: 99.98%, Test AUC: 84.29%\n",
            "Model 3, Epoch: 20, Loss: 0.0119, Train AUC: 100.00%, Test AUC: 84.65%\n",
            "Model 3, Epoch: 30, Loss: 0.0067, Train AUC: 100.00%, Test AUC: 84.64%\n",
            "Model 3, Epoch: 40, Loss: 0.0043, Train AUC: 100.00%, Test AUC: 84.62%\n",
            "Model 3, Epoch: 50, Loss: 0.0030, Train AUC: 100.00%, Test AUC: 84.60%\n",
            "Model 4, Epoch: 10, Loss: 0.1510, Train AUC: 83.42%, Test AUC: 79.03%\n",
            "Model 4, Epoch: 20, Loss: 0.1518, Train AUC: 84.90%, Test AUC: 80.03%\n",
            "Model 4, Epoch: 30, Loss: 0.1505, Train AUC: 85.42%, Test AUC: 80.41%\n",
            "Model 4, Epoch: 40, Loss: 0.1375, Train AUC: 87.32%, Test AUC: 82.91%\n",
            "Model 4, Epoch: 50, Loss: 0.1251, Train AUC: 88.06%, Test AUC: 84.07%\n",
            "New labeled fraud nodes: 0,New labeled non-fraud nodes: 597\n",
            "Total new labeled nodes: 597, remaining test nodes: 2424\n",
            "Time consumption: 256 seconds\n",
            "********* Iteration 4 *********\n",
            "\n",
            "GNN training nodes: 9520\n",
            "Model 1, Epoch: 10, Loss: 0.2011, Train AUC: 81.64%, Test AUC: 79.02%\n",
            "Model 1, Epoch: 20, Loss: 0.2506, Train AUC: 81.96%, Test AUC: 79.21%\n",
            "Model 1, Epoch: 30, Loss: 0.4409, Train AUC: 82.22%, Test AUC: 79.21%\n",
            "Model 1, Epoch: 40, Loss: 0.2804, Train AUC: 82.25%, Test AUC: 79.27%\n",
            "Model 1, Epoch: 50, Loss: 0.2381, Train AUC: 82.35%, Test AUC: 79.31%\n",
            "Model 2, Epoch: 10, Loss: 0.2115, Train AUC: 84.70%, Test AUC: 82.56%\n",
            "Model 2, Epoch: 20, Loss: 0.2098, Train AUC: 85.13%, Test AUC: 82.83%\n",
            "Model 2, Epoch: 30, Loss: 0.2078, Train AUC: 85.38%, Test AUC: 83.01%\n",
            "Model 2, Epoch: 40, Loss: 0.2065, Train AUC: 85.54%, Test AUC: 83.13%\n",
            "Model 2, Epoch: 50, Loss: 0.2057, Train AUC: 85.66%, Test AUC: 83.22%\n",
            "Model 3, Epoch: 10, Loss: 0.0306, Train AUC: 99.95%, Test AUC: 84.68%\n",
            "Model 3, Epoch: 20, Loss: 0.0113, Train AUC: 100.00%, Test AUC: 84.83%\n",
            "Model 3, Epoch: 30, Loss: 0.0063, Train AUC: 100.00%, Test AUC: 84.82%\n",
            "Model 3, Epoch: 40, Loss: 0.0041, Train AUC: 100.00%, Test AUC: 84.80%\n",
            "Model 3, Epoch: 50, Loss: 0.0029, Train AUC: 100.00%, Test AUC: 84.79%\n",
            "Model 4, Epoch: 10, Loss: 0.1455, Train AUC: 82.75%, Test AUC: 79.13%\n",
            "Model 4, Epoch: 20, Loss: 0.1461, Train AUC: 83.28%, Test AUC: 79.49%\n",
            "Model 4, Epoch: 30, Loss: 0.1381, Train AUC: 85.25%, Test AUC: 81.19%\n",
            "Model 4, Epoch: 40, Loss: 0.1280, Train AUC: 86.76%, Test AUC: 82.46%\n",
            "Model 4, Epoch: 50, Loss: 0.1214, Train AUC: 87.35%, Test AUC: 83.45%\n",
            "New labeled fraud nodes: 0,New labeled non-fraud nodes: 262\n",
            "Total new labeled nodes: 262, remaining test nodes: 2162\n",
            "Time consumption: 270 seconds\n",
            "********* Iteration 5 *********\n",
            "\n",
            "GNN training nodes: 9782\n",
            "Model 1, Epoch: 10, Loss: 0.2510, Train AUC: 81.74%, Test AUC: 79.29%\n",
            "Model 1, Epoch: 20, Loss: 0.2809, Train AUC: 81.95%, Test AUC: 79.36%\n",
            "Model 1, Epoch: 30, Loss: 0.2437, Train AUC: 82.09%, Test AUC: 79.35%\n",
            "Model 1, Epoch: 40, Loss: 0.2055, Train AUC: 82.12%, Test AUC: 79.31%\n",
            "Model 1, Epoch: 50, Loss: 0.1799, Train AUC: 82.26%, Test AUC: 79.39%\n",
            "Model 2, Epoch: 10, Loss: 0.1883, Train AUC: 84.07%, Test AUC: 82.24%\n",
            "Model 2, Epoch: 20, Loss: 0.1819, Train AUC: 84.88%, Test AUC: 82.69%\n",
            "Model 2, Epoch: 30, Loss: 0.1815, Train AUC: 85.29%, Test AUC: 82.94%\n",
            "Model 2, Epoch: 40, Loss: 0.1835, Train AUC: 85.50%, Test AUC: 83.10%\n",
            "Model 2, Epoch: 50, Loss: 0.1863, Train AUC: 85.62%, Test AUC: 83.19%\n",
            "Model 3, Epoch: 10, Loss: 0.0310, Train AUC: 99.93%, Test AUC: 85.08%\n",
            "Model 3, Epoch: 20, Loss: 0.0109, Train AUC: 100.00%, Test AUC: 85.11%\n",
            "Model 3, Epoch: 30, Loss: 0.0061, Train AUC: 100.00%, Test AUC: 85.06%\n",
            "Model 3, Epoch: 40, Loss: 0.0040, Train AUC: 100.00%, Test AUC: 85.01%\n",
            "Model 3, Epoch: 50, Loss: 0.0028, Train AUC: 100.00%, Test AUC: 85.01%\n",
            "Model 4, Epoch: 10, Loss: 0.1448, Train AUC: 82.83%, Test AUC: 79.62%\n",
            "Model 4, Epoch: 20, Loss: 0.1427, Train AUC: 82.59%, Test AUC: 79.18%\n",
            "Model 4, Epoch: 30, Loss: 0.1358, Train AUC: 82.72%, Test AUC: 79.27%\n",
            "Model 4, Epoch: 40, Loss: 0.1369, Train AUC: 83.35%, Test AUC: 79.89%\n",
            "Model 4, Epoch: 50, Loss: 0.1280, Train AUC: 84.83%, Test AUC: 80.81%\n",
            "New labeled fraud nodes: 0,New labeled non-fraud nodes: 114\n",
            "Total new labeled nodes: 114, remaining test nodes: 2048\n",
            "Time consumption: 284 seconds\n",
            "********* Iteration 6 *********\n",
            "\n",
            "GNN training nodes: 9896\n",
            "Model 1, Epoch: 10, Loss: 0.6731, Train AUC: 81.84%, Test AUC: 79.47%\n",
            "Model 1, Epoch: 20, Loss: 0.2445, Train AUC: 81.94%, Test AUC: 80.02%\n",
            "Model 1, Epoch: 30, Loss: 0.2457, Train AUC: 81.94%, Test AUC: 80.02%\n",
            "Model 1, Epoch: 40, Loss: 0.2167, Train AUC: 81.99%, Test AUC: 80.05%\n",
            "Model 1, Epoch: 50, Loss: 0.2015, Train AUC: 82.09%, Test AUC: 80.06%\n",
            "Model 2, Epoch: 10, Loss: 0.2138, Train AUC: 83.98%, Test AUC: 83.08%\n",
            "Model 2, Epoch: 20, Loss: 0.2125, Train AUC: 84.63%, Test AUC: 83.47%\n",
            "Model 2, Epoch: 30, Loss: 0.2120, Train AUC: 84.92%, Test AUC: 83.64%\n",
            "Model 2, Epoch: 40, Loss: 0.2118, Train AUC: 85.09%, Test AUC: 83.74%\n",
            "Model 2, Epoch: 50, Loss: 0.2118, Train AUC: 85.20%, Test AUC: 83.81%\n",
            "Model 3, Epoch: 10, Loss: 0.0336, Train AUC: 99.91%, Test AUC: 85.81%\n",
            "Model 3, Epoch: 20, Loss: 0.0115, Train AUC: 100.00%, Test AUC: 85.78%\n",
            "Model 3, Epoch: 30, Loss: 0.0065, Train AUC: 100.00%, Test AUC: 85.74%\n",
            "Model 3, Epoch: 40, Loss: 0.0042, Train AUC: 100.00%, Test AUC: 85.70%\n",
            "Model 3, Epoch: 50, Loss: 0.0029, Train AUC: 100.00%, Test AUC: 85.68%\n",
            "Model 4, Epoch: 10, Loss: 0.1430, Train AUC: 83.12%, Test AUC: 80.22%\n",
            "Model 4, Epoch: 20, Loss: 0.1395, Train AUC: 83.64%, Test AUC: 80.50%\n",
            "Model 4, Epoch: 30, Loss: 0.1334, Train AUC: 84.40%, Test AUC: 81.09%\n",
            "Model 4, Epoch: 40, Loss: 0.1267, Train AUC: 85.84%, Test AUC: 83.03%\n",
            "Model 4, Epoch: 50, Loss: 0.1193, Train AUC: 82.90%, Test AUC: 80.81%\n",
            "New labeled fraud nodes: 0,New labeled non-fraud nodes: 52\n",
            "Total new labeled nodes: 52, remaining test nodes: 1996\n",
            "Time consumption: 282 seconds\n",
            "********* Iteration 7 *********\n",
            "\n",
            "GNN training nodes: 9948\n",
            "Model 1, Epoch: 10, Loss: 0.2288, Train AUC: 81.93%, Test AUC: 79.91%\n",
            "Model 1, Epoch: 20, Loss: 0.2134, Train AUC: 82.01%, Test AUC: 80.01%\n",
            "Model 1, Epoch: 30, Loss: 0.1886, Train AUC: 82.12%, Test AUC: 80.02%\n",
            "Model 1, Epoch: 40, Loss: 0.1696, Train AUC: 82.23%, Test AUC: 80.08%\n",
            "Model 1, Epoch: 50, Loss: 0.1566, Train AUC: 82.34%, Test AUC: 80.10%\n",
            "Model 2, Epoch: 10, Loss: 0.2287, Train AUC: 84.11%, Test AUC: 83.03%\n",
            "Model 2, Epoch: 20, Loss: 0.2201, Train AUC: 84.62%, Test AUC: 83.36%\n",
            "Model 2, Epoch: 30, Loss: 0.2166, Train AUC: 84.90%, Test AUC: 83.54%\n",
            "Model 2, Epoch: 40, Loss: 0.2149, Train AUC: 85.06%, Test AUC: 83.64%\n",
            "Model 2, Epoch: 50, Loss: 0.2140, Train AUC: 85.16%, Test AUC: 83.73%\n",
            "Model 3, Epoch: 10, Loss: 0.0315, Train AUC: 99.92%, Test AUC: 85.72%\n",
            "Model 3, Epoch: 20, Loss: 0.0107, Train AUC: 100.00%, Test AUC: 85.71%\n",
            "Model 3, Epoch: 30, Loss: 0.0060, Train AUC: 100.00%, Test AUC: 85.67%\n",
            "Model 3, Epoch: 40, Loss: 0.0039, Train AUC: 100.00%, Test AUC: 85.64%\n",
            "Model 3, Epoch: 50, Loss: 0.0027, Train AUC: 100.00%, Test AUC: 85.62%\n",
            "Model 4, Epoch: 10, Loss: 0.1439, Train AUC: 82.99%, Test AUC: 80.26%\n",
            "Model 4, Epoch: 20, Loss: 0.1345, Train AUC: 84.05%, Test AUC: 81.35%\n",
            "Model 4, Epoch: 30, Loss: 0.1324, Train AUC: 83.80%, Test AUC: 81.56%\n",
            "Model 4, Epoch: 40, Loss: 0.1322, Train AUC: 84.64%, Test AUC: 82.23%\n",
            "Model 4, Epoch: 50, Loss: 0.1193, Train AUC: 85.58%, Test AUC: 83.92%\n",
            "New labeled fraud nodes: 0,New labeled non-fraud nodes: 19\n",
            "Total new labeled nodes: 19, remaining test nodes: 1977\n",
            "Time consumption: 286 seconds\n",
            "********* Iteration 8 *********\n",
            "\n",
            "GNN training nodes: 9967\n",
            "Model 1, Epoch: 10, Loss: 0.6915, Train AUC: 81.96%, Test AUC: 79.74%\n",
            "Model 1, Epoch: 20, Loss: 0.2520, Train AUC: 81.80%, Test AUC: 80.14%\n",
            "Model 1, Epoch: 30, Loss: 0.2066, Train AUC: 81.99%, Test AUC: 80.31%\n",
            "Model 1, Epoch: 40, Loss: 0.1792, Train AUC: 82.11%, Test AUC: 80.37%\n",
            "Model 1, Epoch: 50, Loss: 0.1791, Train AUC: 82.17%, Test AUC: 80.39%\n",
            "Model 2, Epoch: 10, Loss: 0.2113, Train AUC: 84.42%, Test AUC: 83.37%\n",
            "Model 2, Epoch: 20, Loss: 0.2133, Train AUC: 84.76%, Test AUC: 83.59%\n",
            "Model 2, Epoch: 30, Loss: 0.2133, Train AUC: 84.96%, Test AUC: 83.73%\n",
            "Model 2, Epoch: 40, Loss: 0.2131, Train AUC: 85.08%, Test AUC: 83.82%\n",
            "Model 2, Epoch: 50, Loss: 0.2129, Train AUC: 85.17%, Test AUC: 83.90%\n",
            "Model 3, Epoch: 10, Loss: 0.0335, Train AUC: 99.92%, Test AUC: 85.86%\n",
            "Model 3, Epoch: 20, Loss: 0.0114, Train AUC: 100.00%, Test AUC: 85.81%\n",
            "Model 3, Epoch: 30, Loss: 0.0064, Train AUC: 100.00%, Test AUC: 85.72%\n",
            "Model 3, Epoch: 40, Loss: 0.0042, Train AUC: 100.00%, Test AUC: 85.70%\n",
            "Model 3, Epoch: 50, Loss: 0.0029, Train AUC: 100.00%, Test AUC: 85.68%\n",
            "Model 4, Epoch: 10, Loss: 0.1438, Train AUC: 82.82%, Test AUC: 80.10%\n",
            "Model 4, Epoch: 20, Loss: 0.1428, Train AUC: 83.09%, Test AUC: 80.09%\n",
            "Model 4, Epoch: 30, Loss: 0.1430, Train AUC: 83.46%, Test AUC: 80.27%\n",
            "Model 4, Epoch: 40, Loss: 0.1289, Train AUC: 85.55%, Test AUC: 82.69%\n",
            "Model 4, Epoch: 50, Loss: 0.1219, Train AUC: 87.24%, Test AUC: 84.35%\n",
            "No new nodes can be labeled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check the remaining unlabeled test nodes"
      ],
      "metadata": {
        "id": "6YiGWqXE3vCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "bN80pllwdJZ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fdd0627-3052-49a6-ee68-079e6fc5b250"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1977,)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train on the same ensemble models for remaining unlabeled nodes"
      ],
      "metadata": {
        "id": "lrqQ5vWiuejE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_idx={}\n",
        "split_idx['train']=torch.tensor(np.array(X_train)).to(device)\n",
        "split_idx['test']=torch.tensor(np.array(X_test)).to(device)\n",
        "\n",
        "train_idx = split_idx['test']"
      ],
      "metadata": {
        "id": "_mNkR8wnfEKX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = GCN(in_channels=d,hidden_channels=hidden_channels,out_channels=c,num_layers=num_layers,dropout=dropout,use_bn=False).to(device)\n",
        "optimizer1 = torch.optim.AdamW(model1.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "print(f'Training nodes: {len(X_test)}')\n",
        "for epoch in range(100):       \n",
        "    num_batches = int(len(train_idx) / batch_size) + 1\n",
        "    epoch_loss = 0.0    \n",
        "    for batch in range(num_batches):\n",
        "      i_start = batch * batch_size\n",
        "      i_end = min((batch + 1) * batch_size, len(train_idx))\n",
        "      batch_nodes = train_idx[i_start:i_end]\n",
        "              \n",
        "      model1.train()\n",
        "      optimizer1.zero_grad()\n",
        "      out = model1(dataset)\n",
        "      torch.cuda.empty_cache()\n",
        "      if dataset.label.shape[1] == 1:\n",
        "          true_label = F.one_hot(dataset.label, dataset.label.max() + 1).squeeze(1)\n",
        "      else:\n",
        "          true_label = dataset.label\n",
        "\n",
        "      loss = criterion(out[batch_nodes], true_label.squeeze(1)[batch_nodes].to(torch.float))\n",
        "      torch.cuda.empty_cache()\n",
        "      loss.backward()\n",
        "      torch.cuda.empty_cache()\n",
        "      optimizer1.step()\n",
        "      torch.cuda.empty_cache()\n",
        "      epoch_loss += loss.item() \n",
        "      torch.cuda.empty_cache()\n",
        "    \n",
        "    result = evaluate(model1, dataset, split_idx, eval_func)\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f'Model 1, '\n",
        "        f'Epoch: {epoch+1:02d}, '\n",
        "        f'Loss: {epoch_loss / num_batches:.4f}, '\n",
        "        f'Train AUC: {100 * result[0]:.2f}%, '\n",
        "        f'Test AUC: {100 * result[1]:.2f}%')\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "AYeRRzJKudw_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "602c1a23-f3f9-49d5-e0c1-0126115eb804"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training nodes: 1977\n",
            "Model 1, Epoch: 01, Loss: 12.2427, Train AUC: 77.25%, Test AUC: 76.77%\n",
            "Model 1, Epoch: 02, Loss: 5.2499, Train AUC: 78.68%, Test AUC: 76.82%\n",
            "Model 1, Epoch: 03, Loss: 7.3204, Train AUC: 78.95%, Test AUC: 76.91%\n",
            "Model 1, Epoch: 04, Loss: 6.6649, Train AUC: 79.75%, Test AUC: 78.40%\n",
            "Model 1, Epoch: 05, Loss: 5.8962, Train AUC: 80.33%, Test AUC: 79.10%\n",
            "Model 1, Epoch: 06, Loss: 4.8892, Train AUC: 80.77%, Test AUC: 79.75%\n",
            "Model 1, Epoch: 07, Loss: 3.3992, Train AUC: 81.20%, Test AUC: 80.02%\n",
            "Model 1, Epoch: 08, Loss: 1.9552, Train AUC: 81.67%, Test AUC: 79.93%\n",
            "Model 1, Epoch: 09, Loss: 0.8404, Train AUC: 81.70%, Test AUC: 79.64%\n",
            "Model 1, Epoch: 10, Loss: 0.9637, Train AUC: 81.77%, Test AUC: 79.66%\n",
            "Model 1, Epoch: 11, Loss: 0.8986, Train AUC: 82.82%, Test AUC: 80.65%\n",
            "Model 1, Epoch: 12, Loss: 0.6092, Train AUC: 82.29%, Test AUC: 80.10%\n",
            "Model 1, Epoch: 13, Loss: 0.6383, Train AUC: 82.26%, Test AUC: 80.09%\n",
            "Model 1, Epoch: 14, Loss: 0.5224, Train AUC: 82.63%, Test AUC: 80.58%\n",
            "Model 1, Epoch: 15, Loss: 0.4849, Train AUC: 81.77%, Test AUC: 79.75%\n",
            "Model 1, Epoch: 16, Loss: 0.4513, Train AUC: 80.88%, Test AUC: 78.86%\n",
            "Model 1, Epoch: 17, Loss: 0.4234, Train AUC: 81.41%, Test AUC: 79.72%\n",
            "Model 1, Epoch: 18, Loss: 0.4183, Train AUC: 80.79%, Test AUC: 79.37%\n",
            "Model 1, Epoch: 19, Loss: 0.4115, Train AUC: 80.74%, Test AUC: 79.32%\n",
            "Model 1, Epoch: 20, Loss: 0.4061, Train AUC: 81.44%, Test AUC: 79.82%\n",
            "Model 1, Epoch: 21, Loss: 0.4033, Train AUC: 81.19%, Test AUC: 79.20%\n",
            "Model 1, Epoch: 22, Loss: 0.3995, Train AUC: 81.64%, Test AUC: 79.59%\n",
            "Model 1, Epoch: 23, Loss: 0.3981, Train AUC: 81.71%, Test AUC: 79.65%\n",
            "Model 1, Epoch: 24, Loss: 0.3964, Train AUC: 81.74%, Test AUC: 79.69%\n",
            "Model 1, Epoch: 25, Loss: 0.3955, Train AUC: 81.87%, Test AUC: 79.79%\n",
            "Model 1, Epoch: 26, Loss: 0.3946, Train AUC: 81.87%, Test AUC: 79.79%\n",
            "Model 1, Epoch: 27, Loss: 0.3932, Train AUC: 81.96%, Test AUC: 79.90%\n",
            "Model 1, Epoch: 28, Loss: 0.3929, Train AUC: 82.06%, Test AUC: 80.07%\n",
            "Model 1, Epoch: 29, Loss: 0.3924, Train AUC: 82.11%, Test AUC: 80.04%\n",
            "Model 1, Epoch: 30, Loss: 0.3916, Train AUC: 82.20%, Test AUC: 80.14%\n",
            "Model 1, Epoch: 31, Loss: 0.3912, Train AUC: 82.29%, Test AUC: 80.24%\n",
            "Model 1, Epoch: 32, Loss: 0.3907, Train AUC: 82.36%, Test AUC: 80.25%\n",
            "Model 1, Epoch: 33, Loss: 0.3900, Train AUC: 82.44%, Test AUC: 80.33%\n",
            "Model 1, Epoch: 34, Loss: 0.3895, Train AUC: 82.52%, Test AUC: 80.43%\n",
            "Model 1, Epoch: 35, Loss: 0.3890, Train AUC: 82.60%, Test AUC: 80.50%\n",
            "Model 1, Epoch: 36, Loss: 0.3884, Train AUC: 82.69%, Test AUC: 80.60%\n",
            "Model 1, Epoch: 37, Loss: 0.3878, Train AUC: 82.76%, Test AUC: 80.68%\n",
            "Model 1, Epoch: 38, Loss: 0.3873, Train AUC: 82.84%, Test AUC: 80.83%\n",
            "Model 1, Epoch: 39, Loss: 0.3866, Train AUC: 82.92%, Test AUC: 80.87%\n",
            "Model 1, Epoch: 40, Loss: 0.3860, Train AUC: 83.00%, Test AUC: 81.01%\n",
            "Model 1, Epoch: 41, Loss: 0.3854, Train AUC: 83.06%, Test AUC: 81.07%\n",
            "Model 1, Epoch: 42, Loss: 0.3848, Train AUC: 83.16%, Test AUC: 81.25%\n",
            "Model 1, Epoch: 43, Loss: 0.3841, Train AUC: 83.23%, Test AUC: 81.29%\n",
            "Model 1, Epoch: 44, Loss: 0.3835, Train AUC: 83.31%, Test AUC: 81.45%\n",
            "Model 1, Epoch: 45, Loss: 0.3828, Train AUC: 83.37%, Test AUC: 81.47%\n",
            "Model 1, Epoch: 46, Loss: 0.3823, Train AUC: 83.43%, Test AUC: 81.62%\n",
            "Model 1, Epoch: 47, Loss: 0.3816, Train AUC: 83.50%, Test AUC: 81.61%\n",
            "Model 1, Epoch: 48, Loss: 0.3812, Train AUC: 83.55%, Test AUC: 81.75%\n",
            "Model 1, Epoch: 49, Loss: 0.3805, Train AUC: 83.62%, Test AUC: 81.77%\n",
            "Model 1, Epoch: 50, Loss: 0.3801, Train AUC: 83.66%, Test AUC: 81.84%\n",
            "Model 1, Epoch: 51, Loss: 0.3797, Train AUC: 83.70%, Test AUC: 81.88%\n",
            "Model 1, Epoch: 52, Loss: 0.3794, Train AUC: 83.72%, Test AUC: 81.93%\n",
            "Model 1, Epoch: 53, Loss: 0.3791, Train AUC: 83.75%, Test AUC: 81.98%\n",
            "Model 1, Epoch: 54, Loss: 0.3788, Train AUC: 83.78%, Test AUC: 82.01%\n",
            "Model 1, Epoch: 55, Loss: 0.3787, Train AUC: 83.81%, Test AUC: 82.05%\n",
            "Model 1, Epoch: 56, Loss: 0.3785, Train AUC: 83.85%, Test AUC: 82.07%\n",
            "Model 1, Epoch: 57, Loss: 0.3783, Train AUC: 83.88%, Test AUC: 82.10%\n",
            "Model 1, Epoch: 58, Loss: 0.3781, Train AUC: 83.90%, Test AUC: 82.12%\n",
            "Model 1, Epoch: 59, Loss: 0.3779, Train AUC: 83.92%, Test AUC: 82.14%\n",
            "Model 1, Epoch: 60, Loss: 0.3777, Train AUC: 83.94%, Test AUC: 82.15%\n",
            "Model 1, Epoch: 61, Loss: 0.3776, Train AUC: 83.96%, Test AUC: 82.17%\n",
            "Model 1, Epoch: 62, Loss: 0.3775, Train AUC: 83.97%, Test AUC: 82.20%\n",
            "Model 1, Epoch: 63, Loss: 0.3774, Train AUC: 83.98%, Test AUC: 82.23%\n",
            "Model 1, Epoch: 64, Loss: 0.3773, Train AUC: 84.00%, Test AUC: 82.25%\n",
            "Model 1, Epoch: 65, Loss: 0.3771, Train AUC: 84.01%, Test AUC: 82.27%\n",
            "Model 1, Epoch: 66, Loss: 0.3770, Train AUC: 84.03%, Test AUC: 82.29%\n",
            "Model 1, Epoch: 67, Loss: 0.3769, Train AUC: 84.05%, Test AUC: 82.30%\n",
            "Model 1, Epoch: 68, Loss: 0.3768, Train AUC: 84.06%, Test AUC: 82.33%\n",
            "Model 1, Epoch: 69, Loss: 0.3766, Train AUC: 84.08%, Test AUC: 82.34%\n",
            "Model 1, Epoch: 70, Loss: 0.3765, Train AUC: 84.09%, Test AUC: 82.36%\n",
            "Model 1, Epoch: 71, Loss: 0.3764, Train AUC: 84.11%, Test AUC: 82.37%\n",
            "Model 1, Epoch: 72, Loss: 0.3763, Train AUC: 84.12%, Test AUC: 82.40%\n",
            "Model 1, Epoch: 73, Loss: 0.3762, Train AUC: 84.15%, Test AUC: 82.40%\n",
            "Model 1, Epoch: 74, Loss: 0.3760, Train AUC: 84.16%, Test AUC: 82.42%\n",
            "Model 1, Epoch: 75, Loss: 0.3759, Train AUC: 84.19%, Test AUC: 82.43%\n",
            "Model 1, Epoch: 76, Loss: 0.3758, Train AUC: 84.19%, Test AUC: 82.47%\n",
            "Model 1, Epoch: 77, Loss: 0.3756, Train AUC: 84.22%, Test AUC: 82.48%\n",
            "Model 1, Epoch: 78, Loss: 0.3755, Train AUC: 84.21%, Test AUC: 82.52%\n",
            "Model 1, Epoch: 79, Loss: 0.3754, Train AUC: 84.24%, Test AUC: 82.54%\n",
            "Model 1, Epoch: 80, Loss: 0.3752, Train AUC: 84.25%, Test AUC: 82.56%\n",
            "Model 1, Epoch: 81, Loss: 0.3751, Train AUC: 84.25%, Test AUC: 82.59%\n",
            "Model 1, Epoch: 82, Loss: 0.3750, Train AUC: 84.26%, Test AUC: 82.61%\n",
            "Model 1, Epoch: 83, Loss: 0.3749, Train AUC: 84.28%, Test AUC: 82.62%\n",
            "Model 1, Epoch: 84, Loss: 0.3749, Train AUC: 84.28%, Test AUC: 82.65%\n",
            "Model 1, Epoch: 85, Loss: 0.3748, Train AUC: 84.31%, Test AUC: 82.66%\n",
            "Model 1, Epoch: 86, Loss: 0.3746, Train AUC: 84.32%, Test AUC: 82.68%\n",
            "Model 1, Epoch: 87, Loss: 0.3745, Train AUC: 84.37%, Test AUC: 82.70%\n",
            "Model 1, Epoch: 88, Loss: 0.3742, Train AUC: 84.37%, Test AUC: 82.72%\n",
            "Model 1, Epoch: 89, Loss: 0.3741, Train AUC: 84.41%, Test AUC: 82.75%\n",
            "Model 1, Epoch: 90, Loss: 0.3737, Train AUC: 84.43%, Test AUC: 82.75%\n",
            "Model 1, Epoch: 91, Loss: 0.3736, Train AUC: 84.43%, Test AUC: 82.80%\n",
            "Model 1, Epoch: 92, Loss: 0.3734, Train AUC: 84.47%, Test AUC: 82.79%\n",
            "Model 1, Epoch: 93, Loss: 0.3733, Train AUC: 84.47%, Test AUC: 82.82%\n",
            "Model 1, Epoch: 94, Loss: 0.3731, Train AUC: 84.52%, Test AUC: 82.82%\n",
            "Model 1, Epoch: 95, Loss: 0.3729, Train AUC: 84.53%, Test AUC: 82.85%\n",
            "Model 1, Epoch: 96, Loss: 0.3727, Train AUC: 84.56%, Test AUC: 82.87%\n",
            "Model 1, Epoch: 97, Loss: 0.3724, Train AUC: 84.57%, Test AUC: 82.90%\n",
            "Model 1, Epoch: 98, Loss: 0.3722, Train AUC: 84.59%, Test AUC: 82.92%\n",
            "Model 1, Epoch: 99, Loss: 0.3720, Train AUC: 84.60%, Test AUC: 82.95%\n",
            "Model 1, Epoch: 100, Loss: 0.3719, Train AUC: 84.62%, Test AUC: 82.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model2 = GCN(in_channels=d,hidden_channels=hidden_channels,out_channels=c,num_layers=num_layers,dropout=dropout,use_bn=False).to(device)\n",
        "model2 = SGC(in_channels=d, out_channels=c, hops=1).to(device)\n",
        "optimizer2 = torch.optim.AdamW(model2.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "print(f'Training nodes: {len(train_idx)}')\n",
        "for epoch in range(100):       \n",
        "    num_batches = int(len(train_idx) / batch_size) + 1\n",
        "    epoch_loss = 0.0    \n",
        "    for batch in range(num_batches):\n",
        "      i_start = batch * batch_size\n",
        "      i_end = min((batch + 1) * batch_size, len(train_idx))\n",
        "      batch_nodes = train_idx[i_start:i_end]\n",
        "              \n",
        "      model2.train()\n",
        "      optimizer2.zero_grad()\n",
        "      out = model2(dataset)\n",
        "      if dataset.label.shape[1] == 1:\n",
        "          true_label = F.one_hot(dataset.label, dataset.label.max() + 1).squeeze(1)\n",
        "      else:\n",
        "          true_label = dataset.label\n",
        "\n",
        "      loss = criterion(out[batch_nodes], true_label.squeeze(1)[batch_nodes].to(torch.float))\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer2.step()\n",
        "\n",
        "      epoch_loss += loss.item() \n",
        "  \n",
        "    \n",
        "    result = evaluate(model2, dataset, split_idx, eval_func)\n",
        "    print(f'Model 2, '\n",
        "        f'Epoch: {epoch+1:02d}, '\n",
        "        f'Loss: {epoch_loss / num_batches:.4f}, '\n",
        "        f'Train AUC: {100 * result[0]:.2f}%, '\n",
        "        f'Test AUC: {100 * result[1]:.2f}%')"
      ],
      "metadata": {
        "id": "sy-120yKegSA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0060402-0bea-4d98-a8c7-5ed111fd0b03"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training nodes: 1977\n",
            "Model 2, Epoch: 01, Loss: 49.8101, Train AUC: 37.01%, Test AUC: 42.34%\n",
            "Model 2, Epoch: 02, Loss: 33.2344, Train AUC: 24.22%, Test AUC: 27.66%\n",
            "Model 2, Epoch: 03, Loss: 17.0822, Train AUC: 19.17%, Test AUC: 20.13%\n",
            "Model 2, Epoch: 04, Loss: 5.2705, Train AUC: 71.04%, Test AUC: 71.50%\n",
            "Model 2, Epoch: 05, Loss: 1.7668, Train AUC: 79.21%, Test AUC: 79.53%\n",
            "Model 2, Epoch: 06, Loss: 2.5748, Train AUC: 80.59%, Test AUC: 80.74%\n",
            "Model 2, Epoch: 07, Loss: 3.0570, Train AUC: 81.17%, Test AUC: 81.18%\n",
            "Model 2, Epoch: 08, Loss: 3.2921, Train AUC: 81.51%, Test AUC: 81.39%\n",
            "Model 2, Epoch: 09, Loss: 3.3530, Train AUC: 81.71%, Test AUC: 81.52%\n",
            "Model 2, Epoch: 10, Loss: 3.2898, Train AUC: 81.85%, Test AUC: 81.64%\n",
            "Model 2, Epoch: 11, Loss: 3.1370, Train AUC: 81.97%, Test AUC: 81.77%\n",
            "Model 2, Epoch: 12, Loss: 2.9186, Train AUC: 82.14%, Test AUC: 81.85%\n",
            "Model 2, Epoch: 13, Loss: 2.6513, Train AUC: 82.29%, Test AUC: 81.96%\n",
            "Model 2, Epoch: 14, Loss: 2.3470, Train AUC: 82.45%, Test AUC: 82.06%\n",
            "Model 2, Epoch: 15, Loss: 2.0141, Train AUC: 82.66%, Test AUC: 82.19%\n",
            "Model 2, Epoch: 16, Loss: 1.6588, Train AUC: 82.93%, Test AUC: 82.38%\n",
            "Model 2, Epoch: 17, Loss: 1.2878, Train AUC: 83.31%, Test AUC: 82.60%\n",
            "Model 2, Epoch: 18, Loss: 0.9352, Train AUC: 83.78%, Test AUC: 82.78%\n",
            "Model 2, Epoch: 19, Loss: 0.7958, Train AUC: 84.57%, Test AUC: 82.92%\n",
            "Model 2, Epoch: 20, Loss: 0.5320, Train AUC: 85.25%, Test AUC: 83.00%\n",
            "Model 2, Epoch: 21, Loss: 0.4680, Train AUC: 85.23%, Test AUC: 82.16%\n",
            "Model 2, Epoch: 22, Loss: 0.5064, Train AUC: 85.72%, Test AUC: 82.89%\n",
            "Model 2, Epoch: 23, Loss: 0.4070, Train AUC: 85.94%, Test AUC: 83.31%\n",
            "Model 2, Epoch: 24, Loss: 0.4183, Train AUC: 86.04%, Test AUC: 83.48%\n",
            "Model 2, Epoch: 25, Loss: 0.4228, Train AUC: 86.05%, Test AUC: 83.55%\n",
            "Model 2, Epoch: 26, Loss: 0.4077, Train AUC: 86.11%, Test AUC: 83.62%\n",
            "Model 2, Epoch: 27, Loss: 0.3988, Train AUC: 86.22%, Test AUC: 83.69%\n",
            "Model 2, Epoch: 28, Loss: 0.3970, Train AUC: 86.25%, Test AUC: 83.61%\n",
            "Model 2, Epoch: 29, Loss: 0.3944, Train AUC: 86.20%, Test AUC: 83.54%\n",
            "Model 2, Epoch: 30, Loss: 0.3894, Train AUC: 86.15%, Test AUC: 83.51%\n",
            "Model 2, Epoch: 31, Loss: 0.3879, Train AUC: 86.09%, Test AUC: 83.48%\n",
            "Model 2, Epoch: 32, Loss: 0.3868, Train AUC: 86.06%, Test AUC: 83.40%\n",
            "Model 2, Epoch: 33, Loss: 0.3846, Train AUC: 86.02%, Test AUC: 83.28%\n",
            "Model 2, Epoch: 34, Loss: 0.3837, Train AUC: 85.97%, Test AUC: 83.17%\n",
            "Model 2, Epoch: 35, Loss: 0.3829, Train AUC: 85.98%, Test AUC: 83.19%\n",
            "Model 2, Epoch: 36, Loss: 0.3818, Train AUC: 86.00%, Test AUC: 83.24%\n",
            "Model 2, Epoch: 37, Loss: 0.3815, Train AUC: 86.01%, Test AUC: 83.25%\n",
            "Model 2, Epoch: 38, Loss: 0.3808, Train AUC: 86.02%, Test AUC: 83.23%\n",
            "Model 2, Epoch: 39, Loss: 0.3802, Train AUC: 86.03%, Test AUC: 83.24%\n",
            "Model 2, Epoch: 40, Loss: 0.3796, Train AUC: 86.06%, Test AUC: 83.29%\n",
            "Model 2, Epoch: 41, Loss: 0.3789, Train AUC: 86.09%, Test AUC: 83.34%\n",
            "Model 2, Epoch: 42, Loss: 0.3784, Train AUC: 86.11%, Test AUC: 83.37%\n",
            "Model 2, Epoch: 43, Loss: 0.3779, Train AUC: 86.13%, Test AUC: 83.40%\n",
            "Model 2, Epoch: 44, Loss: 0.3773, Train AUC: 86.15%, Test AUC: 83.42%\n",
            "Model 2, Epoch: 45, Loss: 0.3768, Train AUC: 86.17%, Test AUC: 83.46%\n",
            "Model 2, Epoch: 46, Loss: 0.3763, Train AUC: 86.19%, Test AUC: 83.49%\n",
            "Model 2, Epoch: 47, Loss: 0.3758, Train AUC: 86.21%, Test AUC: 83.51%\n",
            "Model 2, Epoch: 48, Loss: 0.3753, Train AUC: 86.22%, Test AUC: 83.53%\n",
            "Model 2, Epoch: 49, Loss: 0.3748, Train AUC: 86.24%, Test AUC: 83.55%\n",
            "Model 2, Epoch: 50, Loss: 0.3743, Train AUC: 86.25%, Test AUC: 83.57%\n",
            "Model 2, Epoch: 51, Loss: 0.3738, Train AUC: 86.26%, Test AUC: 83.59%\n",
            "Model 2, Epoch: 52, Loss: 0.3733, Train AUC: 86.28%, Test AUC: 83.61%\n",
            "Model 2, Epoch: 53, Loss: 0.3729, Train AUC: 86.29%, Test AUC: 83.63%\n",
            "Model 2, Epoch: 54, Loss: 0.3724, Train AUC: 86.30%, Test AUC: 83.65%\n",
            "Model 2, Epoch: 55, Loss: 0.3720, Train AUC: 86.31%, Test AUC: 83.67%\n",
            "Model 2, Epoch: 56, Loss: 0.3716, Train AUC: 86.32%, Test AUC: 83.69%\n",
            "Model 2, Epoch: 57, Loss: 0.3712, Train AUC: 86.33%, Test AUC: 83.71%\n",
            "Model 2, Epoch: 58, Loss: 0.3708, Train AUC: 86.34%, Test AUC: 83.73%\n",
            "Model 2, Epoch: 59, Loss: 0.3704, Train AUC: 86.35%, Test AUC: 83.75%\n",
            "Model 2, Epoch: 60, Loss: 0.3700, Train AUC: 86.36%, Test AUC: 83.78%\n",
            "Model 2, Epoch: 61, Loss: 0.3696, Train AUC: 86.37%, Test AUC: 83.79%\n",
            "Model 2, Epoch: 62, Loss: 0.3692, Train AUC: 86.38%, Test AUC: 83.81%\n",
            "Model 2, Epoch: 63, Loss: 0.3689, Train AUC: 86.39%, Test AUC: 83.83%\n",
            "Model 2, Epoch: 64, Loss: 0.3685, Train AUC: 86.40%, Test AUC: 83.85%\n",
            "Model 2, Epoch: 65, Loss: 0.3682, Train AUC: 86.40%, Test AUC: 83.87%\n",
            "Model 2, Epoch: 66, Loss: 0.3678, Train AUC: 86.41%, Test AUC: 83.88%\n",
            "Model 2, Epoch: 67, Loss: 0.3675, Train AUC: 86.42%, Test AUC: 83.90%\n",
            "Model 2, Epoch: 68, Loss: 0.3672, Train AUC: 86.43%, Test AUC: 83.92%\n",
            "Model 2, Epoch: 69, Loss: 0.3669, Train AUC: 86.44%, Test AUC: 83.94%\n",
            "Model 2, Epoch: 70, Loss: 0.3666, Train AUC: 86.44%, Test AUC: 83.95%\n",
            "Model 2, Epoch: 71, Loss: 0.3663, Train AUC: 86.45%, Test AUC: 83.97%\n",
            "Model 2, Epoch: 72, Loss: 0.3660, Train AUC: 86.46%, Test AUC: 83.99%\n",
            "Model 2, Epoch: 73, Loss: 0.3658, Train AUC: 86.47%, Test AUC: 84.00%\n",
            "Model 2, Epoch: 74, Loss: 0.3655, Train AUC: 86.48%, Test AUC: 84.01%\n",
            "Model 2, Epoch: 75, Loss: 0.3652, Train AUC: 86.49%, Test AUC: 84.03%\n",
            "Model 2, Epoch: 76, Loss: 0.3650, Train AUC: 86.50%, Test AUC: 84.05%\n",
            "Model 2, Epoch: 77, Loss: 0.3647, Train AUC: 86.50%, Test AUC: 84.06%\n",
            "Model 2, Epoch: 78, Loss: 0.3645, Train AUC: 86.51%, Test AUC: 84.07%\n",
            "Model 2, Epoch: 79, Loss: 0.3643, Train AUC: 86.52%, Test AUC: 84.09%\n",
            "Model 2, Epoch: 80, Loss: 0.3640, Train AUC: 86.52%, Test AUC: 84.11%\n",
            "Model 2, Epoch: 81, Loss: 0.3638, Train AUC: 86.53%, Test AUC: 84.12%\n",
            "Model 2, Epoch: 82, Loss: 0.3636, Train AUC: 86.54%, Test AUC: 84.14%\n",
            "Model 2, Epoch: 83, Loss: 0.3634, Train AUC: 86.55%, Test AUC: 84.15%\n",
            "Model 2, Epoch: 84, Loss: 0.3632, Train AUC: 86.56%, Test AUC: 84.16%\n",
            "Model 2, Epoch: 85, Loss: 0.3630, Train AUC: 86.56%, Test AUC: 84.17%\n",
            "Model 2, Epoch: 86, Loss: 0.3628, Train AUC: 86.57%, Test AUC: 84.18%\n",
            "Model 2, Epoch: 87, Loss: 0.3626, Train AUC: 86.57%, Test AUC: 84.19%\n",
            "Model 2, Epoch: 88, Loss: 0.3624, Train AUC: 86.58%, Test AUC: 84.21%\n",
            "Model 2, Epoch: 89, Loss: 0.3623, Train AUC: 86.58%, Test AUC: 84.22%\n",
            "Model 2, Epoch: 90, Loss: 0.3621, Train AUC: 86.59%, Test AUC: 84.23%\n",
            "Model 2, Epoch: 91, Loss: 0.3619, Train AUC: 86.59%, Test AUC: 84.24%\n",
            "Model 2, Epoch: 92, Loss: 0.3618, Train AUC: 86.60%, Test AUC: 84.25%\n",
            "Model 2, Epoch: 93, Loss: 0.3616, Train AUC: 86.60%, Test AUC: 84.26%\n",
            "Model 2, Epoch: 94, Loss: 0.3614, Train AUC: 86.61%, Test AUC: 84.28%\n",
            "Model 2, Epoch: 95, Loss: 0.3613, Train AUC: 86.61%, Test AUC: 84.29%\n",
            "Model 2, Epoch: 96, Loss: 0.3611, Train AUC: 86.62%, Test AUC: 84.30%\n",
            "Model 2, Epoch: 97, Loss: 0.3610, Train AUC: 86.62%, Test AUC: 84.31%\n",
            "Model 2, Epoch: 98, Loss: 0.3609, Train AUC: 86.63%, Test AUC: 84.31%\n",
            "Model 2, Epoch: 99, Loss: 0.3607, Train AUC: 86.63%, Test AUC: 84.33%\n",
            "Model 2, Epoch: 100, Loss: 0.3606, Train AUC: 86.63%, Test AUC: 84.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model2 = GCN(in_channels=d,hidden_channels=hidden_channels,out_channels=c,num_layers=num_layers,dropout=dropout,use_bn=False).to(device)\n",
        "model3 = LINK(n, c).to(device)\n",
        "optimizer3 = torch.optim.AdamW(model3.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "print(f'Training nodes: {len(train_idx)}')\n",
        "for epoch in range(100):       \n",
        "    num_batches = int(len(train_idx) / batch_size) + 1\n",
        "    epoch_loss = 0.0    \n",
        "    for batch in range(num_batches):\n",
        "      i_start = batch * batch_size\n",
        "      i_end = min((batch + 1) * batch_size, len(train_idx))\n",
        "      batch_nodes = train_idx[i_start:i_end]\n",
        "              \n",
        "      model3.train()\n",
        "      optimizer3.zero_grad()\n",
        "      out = model3(dataset)\n",
        "      if dataset.label.shape[1] == 1:\n",
        "          true_label = F.one_hot(dataset.label, dataset.label.max() + 1).squeeze(1)\n",
        "      else:\n",
        "          true_label = dataset.label\n",
        "\n",
        "      loss = criterion(out[batch_nodes], true_label.squeeze(1)[batch_nodes].to(torch.float))\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer3.step()\n",
        "\n",
        "      epoch_loss += loss.item() \n",
        "  \n",
        "    \n",
        "    result = evaluate(model3, dataset, split_idx, eval_func)\n",
        "    print(f'Model 3, '\n",
        "        f'Epoch: {epoch+1:02d}, '\n",
        "        f'Loss: {epoch_loss / num_batches:.4f}, '\n",
        "        f'Train AUC: {100 * result[0]:.2f}%, '\n",
        "        f'Test AUC: {100 * result[1]:.2f}%')"
      ],
      "metadata": {
        "id": "cRTllDLIlbZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e746191-d747-48a3-f20d-c9139d237d20"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training nodes: 1977\n",
            "Model 3, Epoch: 01, Loss: 0.6608, Train AUC: 80.66%, Test AUC: 79.47%\n",
            "Model 3, Epoch: 02, Loss: 0.7948, Train AUC: 83.05%, Test AUC: 84.29%\n",
            "Model 3, Epoch: 03, Loss: 0.6181, Train AUC: 84.83%, Test AUC: 88.33%\n",
            "Model 3, Epoch: 04, Loss: 0.3953, Train AUC: 84.41%, Test AUC: 91.48%\n",
            "Model 3, Epoch: 05, Loss: 0.3585, Train AUC: 83.60%, Test AUC: 92.37%\n",
            "Model 3, Epoch: 06, Loss: 0.2734, Train AUC: 86.63%, Test AUC: 96.25%\n",
            "Model 3, Epoch: 07, Loss: 0.2031, Train AUC: 86.35%, Test AUC: 97.02%\n",
            "Model 3, Epoch: 08, Loss: 0.1827, Train AUC: 86.22%, Test AUC: 98.42%\n",
            "Model 3, Epoch: 09, Loss: 0.1498, Train AUC: 85.94%, Test AUC: 99.47%\n",
            "Model 3, Epoch: 10, Loss: 0.1297, Train AUC: 85.92%, Test AUC: 99.75%\n",
            "Model 3, Epoch: 11, Loss: 0.1106, Train AUC: 86.47%, Test AUC: 99.91%\n",
            "Model 3, Epoch: 12, Loss: 0.0958, Train AUC: 86.66%, Test AUC: 99.97%\n",
            "Model 3, Epoch: 13, Loss: 0.0881, Train AUC: 86.82%, Test AUC: 99.98%\n",
            "Model 3, Epoch: 14, Loss: 0.0800, Train AUC: 86.95%, Test AUC: 99.98%\n",
            "Model 3, Epoch: 15, Loss: 0.0734, Train AUC: 86.98%, Test AUC: 99.98%\n",
            "Model 3, Epoch: 16, Loss: 0.0682, Train AUC: 86.96%, Test AUC: 99.99%\n",
            "Model 3, Epoch: 17, Loss: 0.0634, Train AUC: 86.92%, Test AUC: 99.99%\n",
            "Model 3, Epoch: 18, Loss: 0.0591, Train AUC: 86.89%, Test AUC: 99.99%\n",
            "Model 3, Epoch: 19, Loss: 0.0555, Train AUC: 86.86%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 20, Loss: 0.0523, Train AUC: 86.85%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 21, Loss: 0.0493, Train AUC: 86.84%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 22, Loss: 0.0467, Train AUC: 86.84%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 23, Loss: 0.0443, Train AUC: 86.84%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 24, Loss: 0.0422, Train AUC: 86.84%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 25, Loss: 0.0402, Train AUC: 86.84%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 26, Loss: 0.0383, Train AUC: 86.84%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 27, Loss: 0.0367, Train AUC: 86.83%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 28, Loss: 0.0351, Train AUC: 86.82%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 29, Loss: 0.0336, Train AUC: 86.82%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 30, Loss: 0.0323, Train AUC: 86.81%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 31, Loss: 0.0310, Train AUC: 86.81%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 32, Loss: 0.0298, Train AUC: 86.80%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 33, Loss: 0.0287, Train AUC: 86.80%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 34, Loss: 0.0276, Train AUC: 86.79%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 35, Loss: 0.0267, Train AUC: 86.78%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 36, Loss: 0.0257, Train AUC: 86.78%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 37, Loss: 0.0248, Train AUC: 86.78%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 38, Loss: 0.0240, Train AUC: 86.77%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 39, Loss: 0.0232, Train AUC: 86.77%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 40, Loss: 0.0225, Train AUC: 86.76%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 41, Loss: 0.0218, Train AUC: 86.76%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 42, Loss: 0.0211, Train AUC: 86.76%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 43, Loss: 0.0205, Train AUC: 86.75%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 44, Loss: 0.0199, Train AUC: 86.75%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 45, Loss: 0.0193, Train AUC: 86.75%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 46, Loss: 0.0187, Train AUC: 86.74%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 47, Loss: 0.0182, Train AUC: 86.74%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 48, Loss: 0.0177, Train AUC: 86.74%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 49, Loss: 0.0172, Train AUC: 86.73%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 50, Loss: 0.0168, Train AUC: 86.73%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 51, Loss: 0.0163, Train AUC: 86.73%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 52, Loss: 0.0159, Train AUC: 86.72%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 53, Loss: 0.0155, Train AUC: 86.72%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 54, Loss: 0.0151, Train AUC: 86.72%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 55, Loss: 0.0148, Train AUC: 86.72%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 56, Loss: 0.0144, Train AUC: 86.71%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 57, Loss: 0.0141, Train AUC: 86.71%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 58, Loss: 0.0137, Train AUC: 86.71%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 59, Loss: 0.0134, Train AUC: 86.71%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 60, Loss: 0.0131, Train AUC: 86.70%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 61, Loss: 0.0128, Train AUC: 86.70%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 62, Loss: 0.0125, Train AUC: 86.70%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 63, Loss: 0.0123, Train AUC: 86.70%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 64, Loss: 0.0120, Train AUC: 86.69%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 65, Loss: 0.0117, Train AUC: 86.69%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 66, Loss: 0.0115, Train AUC: 86.69%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 67, Loss: 0.0113, Train AUC: 86.68%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 68, Loss: 0.0110, Train AUC: 86.68%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 69, Loss: 0.0108, Train AUC: 86.68%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 70, Loss: 0.0106, Train AUC: 86.68%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 71, Loss: 0.0104, Train AUC: 86.68%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 72, Loss: 0.0102, Train AUC: 86.67%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 73, Loss: 0.0100, Train AUC: 86.67%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 74, Loss: 0.0098, Train AUC: 86.67%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 75, Loss: 0.0096, Train AUC: 86.67%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 76, Loss: 0.0094, Train AUC: 86.66%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 77, Loss: 0.0092, Train AUC: 86.66%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 78, Loss: 0.0091, Train AUC: 86.66%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 79, Loss: 0.0089, Train AUC: 86.66%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 80, Loss: 0.0088, Train AUC: 86.65%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 81, Loss: 0.0086, Train AUC: 86.65%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 82, Loss: 0.0084, Train AUC: 86.65%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 83, Loss: 0.0083, Train AUC: 86.65%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 84, Loss: 0.0082, Train AUC: 86.64%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 85, Loss: 0.0080, Train AUC: 86.64%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 86, Loss: 0.0079, Train AUC: 86.64%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 87, Loss: 0.0078, Train AUC: 86.64%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 88, Loss: 0.0076, Train AUC: 86.63%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 89, Loss: 0.0075, Train AUC: 86.63%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 90, Loss: 0.0074, Train AUC: 86.63%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 91, Loss: 0.0073, Train AUC: 86.63%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 92, Loss: 0.0071, Train AUC: 86.63%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 93, Loss: 0.0070, Train AUC: 86.63%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 94, Loss: 0.0069, Train AUC: 86.62%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 95, Loss: 0.0068, Train AUC: 86.62%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 96, Loss: 0.0067, Train AUC: 86.62%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 97, Loss: 0.0066, Train AUC: 86.62%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 98, Loss: 0.0065, Train AUC: 86.62%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 99, Loss: 0.0064, Train AUC: 86.62%, Test AUC: 100.00%\n",
            "Model 3, Epoch: 100, Loss: 0.0063, Train AUC: 86.62%, Test AUC: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model2 = GCN(in_channels=d,hidden_channels=hidden_channels,out_channels=c,num_layers=num_layers,dropout=dropout,use_bn=False).to(device)\n",
        "model4 = LINK(n, c).to(device)\n",
        "optimizer4 = torch.optim.AdamW(model4.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "print(f'Training nodes: {len(train_idx)}')\n",
        "for epoch in range(100):       \n",
        "    num_batches = int(len(train_idx) / batch_size) + 1\n",
        "    epoch_loss = 0.0    \n",
        "    for batch in range(num_batches):\n",
        "      i_start = batch * batch_size\n",
        "      i_end = min((batch + 1) * batch_size, len(train_idx))\n",
        "      batch_nodes = train_idx[i_start:i_end]\n",
        "              \n",
        "      model3.train()\n",
        "      optimizer3.zero_grad()\n",
        "      out = model3(dataset)\n",
        "      if dataset.label.shape[1] == 1:\n",
        "          true_label = F.one_hot(dataset.label, dataset.label.max() + 1).squeeze(1)\n",
        "      else:\n",
        "          true_label = dataset.label\n",
        "\n",
        "      loss = criterion(out[batch_nodes], true_label.squeeze(1)[batch_nodes].to(torch.float))\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer3.step()\n",
        "\n",
        "      epoch_loss += loss.item() \n",
        "  \n",
        "    \n",
        "    result = evaluate(model3, dataset, split_idx, eval_func)\n",
        "    print(f'Model 4, '\n",
        "        f'Epoch: {epoch+1:02d}, '\n",
        "        f'Loss: {epoch_loss / num_batches:.4f}, '\n",
        "        f'Train AUC: {100 * result[0]:.2f}%, '\n",
        "        f'Test AUC: {100 * result[1]:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Mi1kkjOCisc",
        "outputId": "b5be07ef-77ec-4667-df03-7724fb3d1847"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training nodes: 1977\n",
            "Model 4, Epoch: 01, Loss: 0.0062, Train AUC: 86.61%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 02, Loss: 0.0061, Train AUC: 86.61%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 03, Loss: 0.0060, Train AUC: 86.61%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 04, Loss: 0.0060, Train AUC: 86.61%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 05, Loss: 0.0059, Train AUC: 86.61%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 06, Loss: 0.0058, Train AUC: 86.61%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 07, Loss: 0.0057, Train AUC: 86.60%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 08, Loss: 0.0056, Train AUC: 86.60%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 09, Loss: 0.0056, Train AUC: 86.60%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 10, Loss: 0.0055, Train AUC: 86.60%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 11, Loss: 0.0054, Train AUC: 86.60%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 12, Loss: 0.0053, Train AUC: 86.59%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 13, Loss: 0.0053, Train AUC: 86.59%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 14, Loss: 0.0052, Train AUC: 86.59%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 15, Loss: 0.0051, Train AUC: 86.59%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 16, Loss: 0.0051, Train AUC: 86.59%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 17, Loss: 0.0050, Train AUC: 86.59%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 18, Loss: 0.0049, Train AUC: 86.59%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 19, Loss: 0.0049, Train AUC: 86.59%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 20, Loss: 0.0048, Train AUC: 86.58%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 21, Loss: 0.0047, Train AUC: 86.58%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 22, Loss: 0.0047, Train AUC: 86.58%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 23, Loss: 0.0046, Train AUC: 86.58%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 24, Loss: 0.0046, Train AUC: 86.58%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 25, Loss: 0.0045, Train AUC: 86.58%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 26, Loss: 0.0045, Train AUC: 86.58%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 27, Loss: 0.0044, Train AUC: 86.58%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 28, Loss: 0.0044, Train AUC: 86.57%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 29, Loss: 0.0043, Train AUC: 86.57%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 30, Loss: 0.0043, Train AUC: 86.57%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 31, Loss: 0.0042, Train AUC: 86.57%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 32, Loss: 0.0042, Train AUC: 86.57%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 33, Loss: 0.0041, Train AUC: 86.57%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 34, Loss: 0.0041, Train AUC: 86.57%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 35, Loss: 0.0040, Train AUC: 86.57%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 36, Loss: 0.0040, Train AUC: 86.57%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 37, Loss: 0.0039, Train AUC: 86.56%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 38, Loss: 0.0039, Train AUC: 86.56%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 39, Loss: 0.0038, Train AUC: 86.56%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 40, Loss: 0.0038, Train AUC: 86.56%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 41, Loss: 0.0038, Train AUC: 86.56%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 42, Loss: 0.0037, Train AUC: 86.56%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 43, Loss: 0.0037, Train AUC: 86.56%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 44, Loss: 0.0036, Train AUC: 86.56%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 45, Loss: 0.0036, Train AUC: 86.56%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 46, Loss: 0.0036, Train AUC: 86.56%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 47, Loss: 0.0035, Train AUC: 86.55%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 48, Loss: 0.0035, Train AUC: 86.55%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 49, Loss: 0.0035, Train AUC: 86.55%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 50, Loss: 0.0034, Train AUC: 86.55%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 51, Loss: 0.0034, Train AUC: 86.55%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 52, Loss: 0.0033, Train AUC: 86.55%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 53, Loss: 0.0033, Train AUC: 86.55%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 54, Loss: 0.0033, Train AUC: 86.55%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 55, Loss: 0.0032, Train AUC: 86.55%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 56, Loss: 0.0032, Train AUC: 86.55%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 57, Loss: 0.0032, Train AUC: 86.55%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 58, Loss: 0.0031, Train AUC: 86.54%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 59, Loss: 0.0031, Train AUC: 86.54%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 60, Loss: 0.0031, Train AUC: 86.54%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 61, Loss: 0.0031, Train AUC: 86.54%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 62, Loss: 0.0030, Train AUC: 86.54%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 63, Loss: 0.0030, Train AUC: 86.54%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 64, Loss: 0.0030, Train AUC: 86.54%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 65, Loss: 0.0029, Train AUC: 86.54%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 66, Loss: 0.0029, Train AUC: 86.54%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 67, Loss: 0.0029, Train AUC: 86.54%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 68, Loss: 0.0029, Train AUC: 86.54%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 69, Loss: 0.0028, Train AUC: 86.53%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 70, Loss: 0.0028, Train AUC: 86.53%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 71, Loss: 0.0028, Train AUC: 86.53%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 72, Loss: 0.0028, Train AUC: 86.53%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 73, Loss: 0.0027, Train AUC: 86.53%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 74, Loss: 0.0027, Train AUC: 86.53%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 75, Loss: 0.0027, Train AUC: 86.53%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 76, Loss: 0.0027, Train AUC: 86.53%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 77, Loss: 0.0026, Train AUC: 86.53%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 78, Loss: 0.0026, Train AUC: 86.53%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 79, Loss: 0.0026, Train AUC: 86.53%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 80, Loss: 0.0026, Train AUC: 86.53%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 81, Loss: 0.0025, Train AUC: 86.53%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 82, Loss: 0.0025, Train AUC: 86.52%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 83, Loss: 0.0025, Train AUC: 86.52%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 84, Loss: 0.0025, Train AUC: 86.52%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 85, Loss: 0.0025, Train AUC: 86.52%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 86, Loss: 0.0024, Train AUC: 86.52%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 87, Loss: 0.0024, Train AUC: 86.52%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 88, Loss: 0.0024, Train AUC: 86.52%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 89, Loss: 0.0024, Train AUC: 86.52%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 90, Loss: 0.0024, Train AUC: 86.52%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 91, Loss: 0.0023, Train AUC: 86.52%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 92, Loss: 0.0023, Train AUC: 86.52%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 93, Loss: 0.0023, Train AUC: 86.51%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 94, Loss: 0.0023, Train AUC: 86.51%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 95, Loss: 0.0023, Train AUC: 86.51%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 96, Loss: 0.0022, Train AUC: 86.51%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 97, Loss: 0.0022, Train AUC: 86.51%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 98, Loss: 0.0022, Train AUC: 86.51%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 99, Loss: 0.0022, Train AUC: 86.51%, Test AUC: 100.00%\n",
            "Model 4, Epoch: 100, Loss: 0.0022, Train AUC: 86.51%, Test AUC: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result1 = evaluate(model1, dataset, split_idx, eval_func)\n",
        "prob1=result1[2][X_test]\n",
        "prob1=F.softmax(prob1, dim=-1).detach().cpu().numpy()\n",
        "\n",
        "result2 = evaluate(model2, dataset, split_idx, eval_func)\n",
        "prob2=result2[2][X_test]\n",
        "prob2=F.softmax(prob2, dim=-1).detach().cpu().numpy()\n",
        "\n",
        "result3 = evaluate(model3, dataset, split_idx, eval_func)\n",
        "prob3=result3[2][X_test]\n",
        "prob3=F.softmax(prob3, dim=-1).detach().cpu().numpy()\n",
        "\n",
        "result4 = evaluate(model4, dataset, split_idx, eval_func)\n",
        "prob4=result4[2][X_test]\n",
        "prob4=F.softmax(prob4, dim=-1).detach().cpu().numpy()\n",
        "\n",
        "prob_e=(prob1+prob2+prob3+prob4)/4\n",
        "pred=prob_e.argmax(axis=1)\n",
        "\n",
        "#Get true labels for nodes predicted in ensemble models and care-gnn\n",
        "true_before=np.array([label_origin[i] for i in X_train[4777:]])#The first 4777 nodes in X_train are original labeled training nodes\n",
        "true_remain=np.array([label_origin[i] for i in X_test])#Remaining nodes are predicted by care-gnn\n",
        "final_true_all=np.append(true_before,true_remain)\n",
        "#Get all predicted labels\n",
        "final_predict_all=np.append(pseudo_labels,pred)\n",
        "\n",
        "print(classification_report(final_true_all,final_predict_all))"
      ],
      "metadata": {
        "id": "pVms7Zim7AkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "842f1188-c54f-4da9-e40e-4bac3fe5d384"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      1.00      0.98      6674\n",
            "         1.0       0.97      0.47      0.64       493\n",
            "\n",
            "    accuracy                           0.96      7167\n",
            "   macro avg       0.96      0.74      0.81      7167\n",
            "weighted avg       0.96      0.96      0.96      7167\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_probs=np.append(probs,prob_e,axis=0)\n",
        "print(roc_auc_score(final_true_all, final_probs[:,1]))"
      ],
      "metadata": {
        "id": "raycTMJP8Tse",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6258671-9651-40e7-868e-d75b85d7e6d9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9245295691980201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train CARE-GNN for the remaining unlabeled nodes\n",
        "# Reference:https://github.com/YingtongDou/CARE-GNN"
      ],
      "metadata": {
        "id": "-dRmTaQM6hn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "def sparse_to_adjlist(sp_matrix, filename):\n",
        "    \"\"\"\n",
        "    Transfer sparse matrix to adjacency list\n",
        "    :param sp_matrix: the sparse matrix\n",
        "    :param filename: the filename of adjlist\n",
        "    \"\"\"\n",
        "    # add self loop\n",
        "    homo_adj = sp_matrix + sp.eye(sp_matrix.shape[0])\n",
        "    # create adj_list\n",
        "    adj_lists = defaultdict(set)\n",
        "    edges = homo_adj.nonzero()\n",
        "    for index, node in enumerate(edges[0]):\n",
        "        adj_lists[node].add(edges[1][index])\n",
        "        adj_lists[edges[1][index]].add(node)\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(adj_lists, file)\n",
        "    file.close()\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "net_upu = amazon['net_upu']\n",
        "net_usu = amazon['net_usu']\n",
        "net_uvu = amazon['net_uvu']\n",
        "amz_homo = amazon['homo']\n",
        "\n",
        "sparse_to_adjlist(net_upu, prefix_1 + 'amz_upu_adjlists.pickle')\n",
        "sparse_to_adjlist(net_usu, prefix_1 + 'amz_usu_adjlists.pickle')\n",
        "sparse_to_adjlist(net_uvu, prefix_1 + 'amz_uvu_adjlists.pickle')\n",
        "sparse_to_adjlist(amz_homo, prefix_1 + 'amz_homo_adjlists.pickle')"
      ],
      "metadata": {
        "id": "7rCxULfh4ObC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_CARE():\n",
        "    \"\"\"\n",
        "    Load graph, feature, and label given dataset name\n",
        "    :returns: home and single-relation graphs, feature, label\n",
        "    \"\"\"\n",
        "    prefix1 = prefix_2 # Folder where .mat files are stored\n",
        "    prefix2 = prefix_1 # Folder where .pickle files are stored\n",
        "    data_file = loadmat(prefix1 + 'Amazon.mat')\n",
        "    labels = data_file['label'].flatten()\n",
        "    feat_data = data_file['features'].todense().A\n",
        "    # load the preprocessed adj_lists\n",
        "    with open(prefix2 + 'amz_homo_adjlists.pickle', 'rb') as file:\n",
        "        homo = pickle.load(file)\n",
        "    file.close()\n",
        "    with open(prefix2 + 'amz_upu_adjlists.pickle', 'rb') as file:\n",
        "            relation1 = pickle.load(file)\n",
        "    file.close()\n",
        "    with open(prefix2 + 'amz_usu_adjlists.pickle', 'rb') as file:\n",
        "        relation2 = pickle.load(file)\n",
        "    file.close()\n",
        "    with open(prefix2 + 'amz_uvu_adjlists.pickle', 'rb') as file:\n",
        "        relation3 = pickle.load(file)\n",
        "\n",
        "    return [homo, relation1, relation2, relation3], feat_data, labels\n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"\n",
        "    Row-normalize sparse matrix\n",
        "    Code from https://github.com/williamleif/graphsage-simple/\n",
        "    \"\"\"\n",
        "    rowsum = np.array(mx.sum(1)) + 0.01\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def test_care(test_cases, labels, model, batch_size):\n",
        "    \"\"\"\n",
        "    Test the performance of CARE-GNN and its variants\n",
        "    :param test_cases: a list of testing node\n",
        "    :param labels: a list of testing node labels\n",
        "    :param model: the GNN model\n",
        "    :param batch_size: number nodes in a batch\n",
        "    :returns: the AUC and Recall of GNN and Simi modules\n",
        "    \"\"\"\n",
        "\n",
        "    test_batch_num = int(len(test_cases) / batch_size) + 1\n",
        "    f1_gnn = 0.0\n",
        "    acc_gnn = 0.0\n",
        "    recall_gnn = 0.0\n",
        "    f1_label1 = 0.0\n",
        "    acc_label1 = 0.00\n",
        "    recall_label1 = 0.0\n",
        "    gnn_list = []\n",
        "    label_list1 = []\n",
        "    predicted_labels=[]\n",
        "    predict_prob=[]\n",
        "    for iteration in range(test_batch_num):\n",
        "        i_start = iteration * batch_size\n",
        "        i_end = min((iteration + 1) * batch_size, len(test_cases))\n",
        "        batch_nodes = test_cases[i_start:i_end]\n",
        "        batch_label = labels[i_start:i_end]\n",
        "        gnn_prob, label_prob1 = model.to_prob(batch_nodes, batch_label, train_flag=False)\n",
        "\n",
        "        f1_gnn += f1_score(batch_label, gnn_prob.data.cpu().numpy().argmax(axis=1), average=\"macro\")\n",
        "        acc_gnn += accuracy_score(batch_label, gnn_prob.data.cpu().numpy().argmax(axis=1))\n",
        "        recall_gnn += recall_score(batch_label, gnn_prob.data.cpu().numpy().argmax(axis=1), average=\"macro\")\n",
        "\n",
        "        f1_label1 += f1_score(batch_label, label_prob1.data.cpu().numpy().argmax(axis=1), average=\"macro\")\n",
        "        acc_label1 += accuracy_score(batch_label, label_prob1.data.cpu().numpy().argmax(axis=1))\n",
        "        recall_label1 += recall_score(batch_label, label_prob1.data.cpu().numpy().argmax(axis=1), average=\"macro\")\n",
        "\n",
        "        gnn_list.extend(gnn_prob.data.cpu().numpy()[:, 1].tolist())\n",
        "        label_list1.extend(label_prob1.data.cpu().numpy()[:, 1].tolist())\n",
        "        predicted_labels.extend(gnn_prob.data.cpu().numpy().argmax(axis=1))\n",
        "        predict_prob.extend(gnn_prob)\n",
        "    auc_gnn = roc_auc_score(labels, np.array(gnn_list))\n",
        "    ap_gnn = average_precision_score(labels, np.array(gnn_list))\n",
        "    auc_label1 = roc_auc_score(labels, np.array(label_list1))\n",
        "    ap_label1 = average_precision_score(labels, np.array(label_list1))\n",
        "    print(f\"GNN F1: {f1_gnn / test_batch_num:.4f}\")\n",
        "    print(f\"GNN Accuracy: {acc_gnn / test_batch_num:.4f}\")\n",
        "    print(f\"GNN Recall: {recall_gnn / test_batch_num:.4f}\")\n",
        "    print(f\"GNN auc: {auc_gnn:.4f}\")\n",
        "    print(f\"GNN ap: {ap_gnn:.4f}\")\n",
        "    print(f\"Label1 F1: {f1_label1 / test_batch_num:.4f}\")\n",
        "    print(f\"Label1 Accuracy: {acc_label1 / test_batch_num:.4f}\")\n",
        "    print(f\"Label1 Recall: {recall_label1 / test_batch_num:.4f}\")\n",
        "    print(f\"Label1 auc: {auc_label1:.4f}\")\n",
        "    print(f\"Label1 ap: {ap_label1:.4f}\")\n",
        "\n",
        "    return auc_gnn, auc_label1, recall_gnn, recall_label1, predicted_labels,predict_prob"
      ],
      "metadata": {
        "id": "xsJQtGo37RHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InterAgg(nn.Module):\n",
        "\n",
        "    def __init__(self, features, feature_dim,\n",
        "                embed_dim, adj_lists, intraggs,\n",
        "                inter='GNN', step_size=0.02, cuda=True):\n",
        "        \"\"\"\n",
        "        Initialize the inter-relation aggregator\n",
        "        :param features: the input node features or embeddings for all nodes\n",
        "        :param feature_dim: the input dimension\n",
        "        :param embed_dim: the output dimension\n",
        "        :param adj_lists: a list of adjacency lists for each single-relation graph\n",
        "        :param intraggs: the intra-relation aggregators used by each single-relation graph\n",
        "        :param inter: the aggregator type: 'Att', 'Weight', 'Mean', 'GNN'\n",
        "        :param step_size: the RL action step size\n",
        "        :param cuda: whether to use GPU\n",
        "        \"\"\"\n",
        "        super(InterAgg, self).__init__()\n",
        "\n",
        "        self.features = features\n",
        "        self.dropout = 0.6\n",
        "        self.adj_lists = adj_lists\n",
        "        self.intra_agg1 = intraggs[0]\n",
        "        self.intra_agg2 = intraggs[1]\n",
        "        self.intra_agg3 = intraggs[2]\n",
        "        self.embed_dim = embed_dim\n",
        "        self.feat_dim = feature_dim\n",
        "        self.inter = inter\n",
        "        self.step_size = step_size\n",
        "        self.cuda = cuda\n",
        "        self.intra_agg1.cuda = cuda\n",
        "        self.intra_agg2.cuda = cuda\n",
        "        self.intra_agg3.cuda = cuda\n",
        "\n",
        "        # RL condition flag\n",
        "        self.RL = True\n",
        "\n",
        "        # number of batches for current epoch, assigned during training\n",
        "        self.batch_num = 0\n",
        "\n",
        "        # initial filtering thresholds\n",
        "        self.thresholds = [0.5, 0.5, 0.5]\n",
        "\n",
        "        # the activation function used by attention mechanism\n",
        "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
        "\n",
        "        # parameter used to transform node embeddings before inter-relation aggregation\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(self.feat_dim, self.embed_dim))\n",
        "        init.xavier_uniform_(self.weight)\n",
        "\n",
        "        # weight parameter for each relation used by CARE-Weight\n",
        "        self.alpha = nn.Parameter(torch.FloatTensor(self.embed_dim, 3))\n",
        "        init.xavier_uniform_(self.alpha)\n",
        "\n",
        "        # parameters used by attention layer\n",
        "        self.a = nn.Parameter(torch.FloatTensor(2 * self.embed_dim, 1))\n",
        "        init.xavier_uniform_(self.a)\n",
        "\n",
        "        # label predictor for similarity measure\n",
        "        self.label_clf = nn.Linear(self.feat_dim, 2)\n",
        "\n",
        "        # initialize the parameter logs\n",
        "        self.weights_log = []\n",
        "        self.thresholds_log = [self.thresholds]\n",
        "        self.relation_score_log = []\n",
        "        \n",
        "    def forward(self, nodes, labels, train_flag=True):\n",
        "        \"\"\"\n",
        "        :param nodes: a list of batch node ids\n",
        "        :param labels: a list of batch node labels, only used by the RLModule\n",
        "        :param train_flag: indicates whether in training or testing mode\n",
        "        :return combined: the embeddings of a batch of input node features\n",
        "        :return center_scores: the label-aware scores of batch nodes\n",
        "        \"\"\"\n",
        "\n",
        "    # extract 1-hop neighbor ids from adj lists of each single-relation graph\n",
        "        to_neighs = []\n",
        "        for adj_list in self.adj_lists:\n",
        "            to_neighs.append([set(adj_list[int(node)]) for node in nodes])\n",
        "\n",
        "        # find unique nodes and their neighbors used in current batch\n",
        "        unique_nodes = set.union(set.union(*to_neighs[0]), set.union(*to_neighs[1]),\n",
        "                                 set.union(*to_neighs[2], set(nodes)))\n",
        "\n",
        "        # calculate label-aware scores\n",
        "        if self.cuda:\n",
        "            batch_features = self.features(torch.cuda.LongTensor(list(unique_nodes)))\n",
        "        else:\n",
        "            batch_features = self.features(torch.LongTensor(list(unique_nodes)))\n",
        "        batch_scores = self.label_clf(batch_features)\n",
        "        id_mapping = {node_id: index for node_id, index in zip(unique_nodes, range(len(unique_nodes)))}\n",
        "\n",
        "        # the label-aware scores for current batch of nodes\n",
        "        center_scores = batch_scores[itemgetter(*nodes)(id_mapping), :]\n",
        "\n",
        "        # get neighbor node id list for each batch node and relation\n",
        "        r1_list = [list(to_neigh) for to_neigh in to_neighs[0]]\n",
        "        r2_list = [list(to_neigh) for to_neigh in to_neighs[1]]\n",
        "        r3_list = [list(to_neigh) for to_neigh in to_neighs[2]]\n",
        "\n",
        "        # assign label-aware scores to neighbor nodes for each batch node and relation\n",
        "        r1_scores = [batch_scores[itemgetter(*to_neigh)(id_mapping), :].view(-1, 2) for to_neigh in r1_list]\n",
        "        r2_scores = [batch_scores[itemgetter(*to_neigh)(id_mapping), :].view(-1, 2) for to_neigh in r2_list]\n",
        "        r3_scores = [batch_scores[itemgetter(*to_neigh)(id_mapping), :].view(-1, 2) for to_neigh in r3_list]\n",
        "\n",
        "        # count the number of neighbors kept for aggregation for each batch node and relation\n",
        "        r1_sample_num_list = [math.ceil(len(neighs) * self.thresholds[0]) for neighs in r1_list]\n",
        "        r2_sample_num_list = [math.ceil(len(neighs) * self.thresholds[1]) for neighs in r2_list]\n",
        "        r3_sample_num_list = [math.ceil(len(neighs) * self.thresholds[2]) for neighs in r3_list]\n",
        "\n",
        "        # intra-aggregation steps for each relation\n",
        "        # Eq. (8) in the paper\n",
        "        r1_feats, r1_scores = self.intra_agg1.forward(nodes, r1_list, center_scores, r1_scores, r1_sample_num_list)\n",
        "        r2_feats, r2_scores = self.intra_agg2.forward(nodes, r2_list, center_scores, r2_scores, r2_sample_num_list)\n",
        "        r3_feats, r3_scores = self.intra_agg3.forward(nodes, r3_list, center_scores, r3_scores, r3_sample_num_list)\n",
        "\n",
        "        # concat the intra-aggregated embeddings from each relation\n",
        "        neigh_feats = torch.cat((r1_feats, r2_feats, r3_feats), dim=0)\n",
        "\n",
        "        # get features or embeddings for batch nodes\n",
        "        if self.cuda and isinstance(nodes, list):\n",
        "            index = torch.LongTensor(nodes).cuda()\n",
        "        else:\n",
        "            index = torch.LongTensor(nodes)\n",
        "        self_feats = self.features(index)\n",
        "\n",
        "        # number of nodes in a batch\n",
        "        n = len(nodes)\n",
        "\n",
        "        # inter-relation aggregation steps\n",
        "        # Eq. (9) in the paper\n",
        "        if self.inter == 'Att':\n",
        "            # 1) CARE-Att Inter-relation Aggregator\n",
        "            combined, attention = att_inter_agg(len(self.adj_lists), self.leakyrelu, self_feats, neigh_feats, self.embed_dim,\n",
        "                                                self.weight, self.a, n, self.dropout, self.training, self.cuda)\n",
        "        elif self.inter == 'Weight':\n",
        "            # 2) CARE-Weight Inter-relation Aggregator\n",
        "            combined = weight_inter_agg(len(self.adj_lists), self_feats, neigh_feats, self.embed_dim, self.weight, self.alpha, n, self.cuda)\n",
        "            gem_weights = F.softmax(torch.sum(self.alpha, dim=0), dim=0).tolist()\n",
        "            if train_flag:\n",
        "                print(f'Weights: {gem_weights}')\n",
        "        elif self.inter == 'Mean':\n",
        "            # 3) CARE-Mean Inter-relation Aggregator\n",
        "            combined = mean_inter_agg(len(self.adj_lists), self_feats, neigh_feats, self.embed_dim, self.weight, n, self.cuda)\n",
        "        elif self.inter == 'GNN':\n",
        "            # 4) CARE-GNN Inter-relation Aggregator\n",
        "            combined = threshold_inter_agg(len(self.adj_lists), self_feats, neigh_feats, self.embed_dim, self.weight, self.thresholds, n, self.cuda)\n",
        "\n",
        "        # the reinforcement learning module\n",
        "        if self.RL and train_flag:\n",
        "            relation_scores, rewards, thresholds, stop_flag = RLModule([r1_scores, r2_scores, r3_scores],\n",
        "                                                                        self.relation_score_log, labels, self.thresholds,\n",
        "                                                                        self.batch_num, self.step_size)\n",
        "            self.thresholds = thresholds\n",
        "            self.RL = stop_flag\n",
        "            self.relation_score_log.append(relation_scores)\n",
        "            self.thresholds_log.append(self.thresholds)\n",
        "\n",
        "        return combined, center_scores\n",
        "\n",
        "class IntraAgg(nn.Module):\n",
        "\n",
        "    def __init__(self, features, feat_dim, cuda=False):\n",
        "        \"\"\"\n",
        "        Initialize the intra-relation aggregator\n",
        "        :param features: the input node features or embeddings for all nodes\n",
        "        :param feat_dim: the input dimension\n",
        "        :param cuda: whether to use GPU\n",
        "        \"\"\"\n",
        "        super(IntraAgg, self).__init__()\n",
        "\n",
        "        self.features = features\n",
        "        self.cuda = cuda\n",
        "        self.feat_dim = feat_dim\n",
        "\n",
        "    def forward(self, nodes, to_neighs_list, batch_scores, neigh_scores, sample_list):\n",
        "        \"\"\"\n",
        "        Code partially from https://github.com/williamleif/graphsage-simple/\n",
        "        :param nodes: list of nodes in a batch\n",
        "        :param to_neighs_list: neighbor node id list for each batch node in one relation\n",
        "        :param batch_scores: the label-aware scores of batch nodes\n",
        "        :param neigh_scores: the label-aware scores 1-hop neighbors each batch node in one relation\n",
        "        :param sample_list: the number of neighbors kept for each batch node in one relation\n",
        "        :return to_feats: the aggregated embeddings of batch nodes neighbors in one relation\n",
        "        :return samp_scores: the average neighbor distances for each relation after filtering\n",
        "        \"\"\"\n",
        "\n",
        "        # filer neighbors under given relation\n",
        "        samp_neighs, samp_scores = filter_neighs_ada_threshold(batch_scores, neigh_scores, to_neighs_list, sample_list)\n",
        "\n",
        "        # find the unique nodes among batch nodes and the filtered neighbors\n",
        "        unique_nodes_list = list(set.union(*samp_neighs))\n",
        "        unique_nodes = {n: i for i, n in enumerate(unique_nodes_list)}\n",
        "\n",
        "        # intra-relation aggregation only with sampled neighbors\n",
        "        mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))\n",
        "        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]\n",
        "        row_indices = [i for i in range(len(samp_neighs)) for _ in range(len(samp_neighs[i]))]\n",
        "        mask[row_indices, column_indices] = 1\n",
        "        if self.cuda:\n",
        "            mask = mask.cuda()\n",
        "        num_neigh = mask.sum(1, keepdim=True)\n",
        "        mask = mask.div(num_neigh)\n",
        "        if self.cuda:\n",
        "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())\n",
        "        else:\n",
        "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list))\n",
        "        to_feats = mask.mm(embed_matrix)\n",
        "        to_feats = F.relu(to_feats)\n",
        "        return to_feats, samp_scores\n",
        "\n",
        "def RLModule(scores, scores_log, labels, thresholds, batch_num, step_size):\n",
        "    \"\"\"\n",
        "    The reinforcement learning module.\n",
        "    It updates the neighbor filtering threshold for each relation based\n",
        "    on the average neighbor distances between two consecutive epochs.\n",
        "    :param scores: the neighbor nodes label-aware scores for each relation\n",
        "    :param scores_log: a list stores the relation average distances for each batch\n",
        "    :param labels: the batch node labels used to select positive nodes\n",
        "    :param thresholds: the current neighbor filtering thresholds for each relation\n",
        "    :param batch_num: numbers batches in an epoch\n",
        "    :param step_size: the RL action step size\n",
        "    :return relation_scores: the relation average distances for current batch\n",
        "    :return rewards: the reward for given thresholds in current epoch\n",
        "    :return new_thresholds: the new filtering thresholds updated according to the rewards\n",
        "    :return stop_flag: the RL terminal condition flag\n",
        "    \"\"\"\n",
        "\n",
        "    relation_scores = []\n",
        "    stop_flag = True\n",
        "    # only compute the average neighbor distances for positive nodes\n",
        "    pos_index = (labels == 1).nonzero().tolist()\n",
        "    pos_index = [i[0] for i in pos_index]\n",
        "\n",
        "    # compute average neighbor distances for each relation\n",
        "    for score in scores:\n",
        "        pos_scores = itemgetter(*pos_index)(score)\n",
        "        neigh_count = sum([1 if isinstance(i, float) else len(i) for i in pos_scores])\n",
        "        pos_sum = [i if isinstance(i, float) else sum(i) for i in pos_scores]\n",
        "        relation_scores.append(sum(pos_sum) / neigh_count)\n",
        "\n",
        "    if len(scores_log) % batch_num != 0 or len(scores_log) < 2 * batch_num:\n",
        "        # do not call RL module within the epoch or within the first two epochs\n",
        "        rewards = [0, 0, 0]\n",
        "        new_thresholds = thresholds\n",
        "    else:\n",
        "        # update thresholds according to average scores in last epoch\n",
        "        # Eq.(5) in the paper\n",
        "        previous_epoch_scores = [sum(s) / batch_num for s in zip(*scores_log[-2 * batch_num:-batch_num])]\n",
        "        current_epoch_scores = [sum(s) / batch_num for s in zip(*scores_log[-batch_num:])]\n",
        "\n",
        "        # compute reward for each relation and update the thresholds according to reward\n",
        "        # Eq. (6) in the paper\n",
        "        rewards = [1 if previous_epoch_scores[i] - s >= 0 else -1 for i, s in enumerate(current_epoch_scores)]\n",
        "        new_thresholds = [thresholds[i] + step_size if r == 1 else thresholds[i] - step_size for i, r in enumerate(rewards)]\n",
        "\n",
        "        # avoid overflow\n",
        "        new_thresholds = [0.999 if i > 1 else i for i in new_thresholds]\n",
        "        new_thresholds = [0.001 if i < 0 else i for i in new_thresholds]\n",
        "\n",
        "        print(f'epoch scores: {current_epoch_scores}')\n",
        "        print(f'rewards: {rewards}')\n",
        "        print(f'thresholds: {new_thresholds}')\n",
        "\n",
        "    # TODO: add terminal condition\n",
        "\n",
        "    return relation_scores, rewards, new_thresholds, stop_flag\n",
        "\n",
        "def filter_neighs_ada_threshold(center_scores, neigh_scores, neighs_list, sample_list):\n",
        "    \"\"\"\n",
        "    Filter neighbors according label predictor result with adaptive thresholds\n",
        "    :param center_scores: the label-aware scores of batch nodes\n",
        "    :param neigh_scores: the label-aware scores 1-hop neighbors each batch node in one relation\n",
        "    :param neighs_list: neighbor node id list for each batch node in one relation\n",
        "    :param sample_list: the number of neighbors kept for each batch node in one relation\n",
        "    :return samp_neighs: the neighbor indices and neighbor simi scores\n",
        "    :return samp_scores: the average neighbor distances for each relation after filtering\n",
        "    \"\"\"\n",
        "\n",
        "    samp_neighs = []\n",
        "    samp_scores = []\n",
        "    for idx, center_score in enumerate(center_scores):\n",
        "        center_score = center_scores[idx][0]\n",
        "        neigh_score = neigh_scores[idx][:, 0].view(-1, 1)\n",
        "        center_score = center_score.repeat(neigh_score.size()[0], 1)\n",
        "        neighs_indices = neighs_list[idx]\n",
        "        num_sample = sample_list[idx]\n",
        "\n",
        "        # compute the L1-distance of batch nodes and their neighbors\n",
        "        # Eq. (2) in paper\n",
        "        score_diff = torch.abs(center_score - neigh_score).squeeze()\n",
        "        sorted_scores, sorted_indices = torch.sort(score_diff, dim=0, descending=False)\n",
        "        selected_indices = sorted_indices.tolist()\n",
        "\n",
        "        # top-p sampling according to distance ranking and thresholds\n",
        "        # Section 3.3.1 in paper\n",
        "        if len(neigh_scores[idx]) > num_sample + 1:\n",
        "            selected_neighs = [neighs_indices[n] for n in selected_indices[:num_sample]]\n",
        "            selected_scores = sorted_scores.tolist()[:num_sample]\n",
        "        else:\n",
        "            selected_neighs = neighs_indices\n",
        "            selected_scores = score_diff.tolist()\n",
        "            if isinstance(selected_scores, float):\n",
        "                selected_scores = [selected_scores]\n",
        "\n",
        "        samp_neighs.append(set(selected_neighs))\n",
        "        samp_scores.append(selected_scores)\n",
        "\n",
        "    return samp_neighs, samp_scores\n",
        "\n",
        "def mean_inter_agg(num_relations, self_feats, neigh_feats, embed_dim, weight, n, cuda):\n",
        "    \"\"\"\n",
        "    Mean inter-relation aggregator\n",
        "    :param num_relations: number of relations in the graph\n",
        "    :param self_feats: batch nodes features or embeddings\n",
        "    :param neigh_feats: intra-relation aggregated neighbor embeddings for each relation\n",
        "    :param embed_dim: the dimension of output embedding\n",
        "    :param weight: parameter used to transform node embeddings before inter-relation aggregation\n",
        "    :param n: number of nodes in a batch\n",
        "    :param cuda: whether use GPU\n",
        "    :return: inter-relation aggregated node embeddings\n",
        "    \"\"\"\n",
        "\n",
        "# transform batch node embedding and neighbor embedding in each relation with weight parameter\n",
        "    center_h = torch.mm(self_feats, weight)\n",
        "    neigh_h = torch.mm(neigh_feats, weight)\n",
        "\n",
        "    # initialize the final neighbor embedding\n",
        "    if cuda:\n",
        "        aggregated = torch.zeros(size=(n, embed_dim)).cuda()\n",
        "    else:\n",
        "        aggregated = torch.zeros(size=(n, embed_dim))\n",
        "\n",
        "    # sum neighbor embeddings together\n",
        "    for r in range(num_relations):\n",
        "        aggregated += neigh_h[r * n:(r + 1) * n, :]\n",
        "\n",
        "    # sum aggregated neighbor embedding and batch node embedding\n",
        "    # take the average of embedding and feed them to activation function\n",
        "    combined = F.relu((center_h + aggregated) / 4.0)\n",
        "\n",
        "    return combined\n",
        "\n",
        "def weight_inter_agg(num_relations, self_feats, neigh_feats, embed_dim, weight, alpha, n, cuda):\n",
        "    \"\"\"\n",
        "    Weight inter-relation aggregator\n",
        "    Reference: https://arxiv.org/abs/2002.12307\n",
        "    :param num_relations: number of relations in the graph\n",
        "    :param self_feats: batch nodes features or embeddings\n",
        "    :param neigh_feats: intra-relation aggregated neighbor embeddings for each relation\n",
        "    :param embed_dim: the dimension of output embedding\n",
        "    :param weight: parameter used to transform node embeddings before inter-relation aggregation\n",
        "    :param alpha: weight parameter for each relation used by CARE-Weight\n",
        "    :param n: number of nodes in a batch\n",
        "    :param cuda: whether use GPU\n",
        "    :return: inter-relation aggregated node embeddings\n",
        "    \"\"\"\n",
        "\n",
        "    # transform batch node embedding and neighbor embedding in each relation with weight parameter\n",
        "    center_h = torch.mm(self_feats, weight)\n",
        "    neigh_h = torch.mm(neigh_feats, weight)\n",
        "\n",
        "    # compute relation weights using softmax\n",
        "    w = F.softmax(alpha, dim=1)\n",
        "\n",
        "    # initialize the final neighbor embedding\n",
        "    if cuda:\n",
        "        aggregated = torch.zeros(size=(n, embed_dim)).cuda()\n",
        "    else:\n",
        "        aggregated = torch.zeros(size=(n, embed_dim))\n",
        "\n",
        "    # add weighted neighbor embeddings in each relation together\n",
        "    for r in range(num_relations):\n",
        "        aggregated += neigh_h[r * n:(r + 1) * n, :] * w[:, r]\n",
        "\n",
        "    # sum aggregated neighbor embedding and batch node embedding\n",
        "    # feed them to activation function\n",
        "    combined = F.relu(center_h + aggregated)\n",
        "\n",
        "    return combined\n",
        "\n",
        "def att_inter_agg(num_relations, att_layer, self_feats, neigh_feats, embed_dim, weight, a, n, dropout, training, cuda):\n",
        "    \"\"\"\n",
        "    Attention-based inter-relation aggregator\n",
        "    Reference: https://github.com/Diego999/pyGAT\n",
        "    :param num_relations: num_relations: number of relations in the graph\n",
        "    :param att_layer: the activation function used by the attention layer\n",
        "    :param self_feats: batch nodes features or embeddings\n",
        "    :param neigh_feats: intra-relation aggregated neighbor embeddings for each relation\n",
        "    :param embed_dim: the dimension of output embedding\n",
        "    :param weight: parameter used to transform node embeddings before inter-relation aggregation\n",
        "    :param a: parameters used by attention layer\n",
        "    :param n: number of nodes in a batch\n",
        "    :param dropout: dropout for attention layer\n",
        "    :param training: a flag indicating whether in the training or testing mode\n",
        "    :param cuda: whether use GPU\n",
        "    :return combined: inter-relation aggregated node embeddings\n",
        "    :return att: the attention weights for each relation\n",
        "    \"\"\"\n",
        "\n",
        "    # transform batch node embedding and neighbor embedding in each relation with weight parameter\n",
        "    center_h = torch.mm(self_feats, weight)\n",
        "    neigh_h = torch.mm(neigh_feats, weight)\n",
        "\n",
        "    import pdb\n",
        "    pdb.set_trace()\n",
        "    # compute attention weights\n",
        "    combined = torch.cat((center_h.repeat(3, 1), neigh_h), dim=1)\n",
        "    e = att_layer(combined.mm(a))\n",
        "    attention = torch.cat((e[0:n, :], e[n:2 * n, :], e[2 * n:3 * n, :]), dim=1)\n",
        "    ori_attention = F.softmax(attention, dim=1)\n",
        "    attention = F.dropout(ori_attention, dropout, training=training)\n",
        "\n",
        "    # initialize the final neighbor embedding\n",
        "    if cuda:\n",
        "        aggregated = torch.zeros(size=(n, embed_dim)).cuda()\n",
        "    else:\n",
        "        aggregated = torch.zeros(size=(n, embed_dim))\n",
        "\n",
        "    # add neighbor embeddings in each relation together with attention weights\n",
        "    for r in range(num_relations):\n",
        "        aggregated += torch.mul(attention[:, r].unsqueeze(1).repeat(1, embed_dim), neigh_h[r * n:(r + 1) * n, :])\n",
        "\n",
        "    # sum aggregated neighbor embedding and batch node embedding\n",
        "    # feed them to activation function\n",
        "    combined = F.relu((center_h + aggregated))\n",
        "\n",
        "    # extract the attention weights\n",
        "    att = F.softmax(torch.sum(ori_attention, dim=0), dim=0)\n",
        "\n",
        "    return combined, att\n",
        "\n",
        "def threshold_inter_agg(num_relations, self_feats, neigh_feats, embed_dim, weight, threshold, n, cuda):\n",
        "    \"\"\"\n",
        "    CARE-GNN inter-relation aggregator\n",
        "    Eq. (9) in the paper\n",
        "    :param num_relations: number of relations in the graph\n",
        "    :param self_feats: batch nodes features or embeddings\n",
        "    :param neigh_feats: intra-relation aggregated neighbor embeddings for each relation\n",
        "    :param embed_dim: the dimension of output embedding\n",
        "    :param weight: parameter used to transform node embeddings before inter-relation aggregation\n",
        "    :param threshold: the neighbor filtering thresholds used as aggregating weights\n",
        "    :param n: number of nodes in a batch\n",
        "    :param cuda: whether use GPU\n",
        "    :return: inter-relation aggregated node embeddings\n",
        "    \"\"\"\n",
        "\n",
        "    # transform batch node embedding and neighbor embedding in each relation with weight parameter\n",
        "    center_h = torch.mm(self_feats, weight)\n",
        "    neigh_h = torch.mm(neigh_feats, weight)\n",
        "\n",
        "    # initialize the final neighbor embedding\n",
        "    if cuda:\n",
        "        aggregated = torch.zeros(size=(n, embed_dim)).cuda()\n",
        "    else:\n",
        "        aggregated = torch.zeros(size=(n, embed_dim))\n",
        "\n",
        "    # add weighted neighbor embeddings in each relation together\n",
        "    for r in range(num_relations):\n",
        "        aggregated += neigh_h[r * n:(r + 1) * n, :] * threshold[r]\n",
        "\n",
        "    # sum aggregated neighbor embedding and batch node embedding\n",
        "    # feed them to activation function\n",
        "    combined = F.relu(center_h + aggregated)\n",
        "\n",
        "    return combined"
      ],
      "metadata": {
        "id": "wHpSGaGc7ROg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CARE-GNN model\n",
        "class OneLayerCARE(nn.Module):\n",
        "    \"\"\"\n",
        "    The CARE-GNN model in one layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, inter1, lambda_1):\n",
        "        \"\"\"\n",
        "        Initialize the CARE-GNN model\n",
        "        :param num_classes: number of classes (2 in our paper)\n",
        "        :param inter1: the inter-relation aggregator that output the final embedding\n",
        "        \"\"\"\n",
        "        super(OneLayerCARE, self).__init__()\n",
        "        self.inter1 = inter1\n",
        "        self.xent = nn.CrossEntropyLoss()\n",
        "\n",
        "        # the parameter to transform the final embedding\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(inter1.embed_dim, num_classes))\n",
        "        init.xavier_uniform_(self.weight)\n",
        "        self.lambda_1 = lambda_1\n",
        "\n",
        "    def forward(self, nodes, labels, train_flag=True):\n",
        "        embeds1, label_scores = self.inter1(nodes, labels, train_flag)\n",
        "        scores = torch.mm(embeds1, self.weight)\n",
        "        return scores, label_scores\n",
        "\n",
        "    def to_prob(self, nodes, labels, train_flag=True):\n",
        "        gnn_scores, label_scores = self.forward(nodes, labels, train_flag)\n",
        "        gnn_prob = nn.functional.softmax(gnn_scores, dim=1)\n",
        "        label_prob = nn.functional.softmax(label_scores, dim=1)\n",
        "        return gnn_prob, label_prob\n",
        "\n",
        "    def loss(self, nodes, labels, train_flag=True):\n",
        "        gnn_scores, label_scores = self.forward(nodes, labels, train_flag)\n",
        "        # Simi loss, Eq. (4) in the paper\n",
        "        label_loss = self.xent(label_scores, labels.squeeze())\n",
        "        # GNN loss, Eq. (10) in the paper\n",
        "        gnn_loss = self.xent(gnn_scores, labels.squeeze())\n",
        "        # the loss function of CARE-GNN, Eq. (11) in the paper\n",
        "        final_loss = gnn_loss + self.lambda_1 * label_loss\n",
        "        return final_loss"
      ],
      "metadata": {
        "id": "iFxt3Mcv7RSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training settings for CARE-GNN\n",
        "data='Amazon'\n",
        "model='CARE'\n",
        "inter='GNN' #The inter-relation aggregator type. [Att, Weight, Mean, GNN]\n",
        "#batch_size=1024\n",
        "lr=0.01\n",
        "lambda_1=2 #Simi loss weight\n",
        "lambda_2=1e-3 #Weight decay (L2 loss weight)\n",
        "emb_size=64 #Node embedding size at the last layer\n",
        "step_size=2e-2 #Reinforcement Learning action step size\n",
        "cuda=False#torch.cuda.is_available()\n",
        "np.random.seed(66)\n",
        "random.seed(66)"
      ],
      "metadata": {
        "id": "xYLvhTv386XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load graph, feature, and label\n",
        "[homo, relation1, relation2, relation3], feat_data, labels = load_data_CARE()\n"
      ],
      "metadata": {
        "id": "33DOxCVG86ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The labels are original True labels, need to update to our pseudo labels\n",
        "for i in range(len(X_train)):\n",
        "  labels[X_train[i]]=y_train[i]"
      ],
      "metadata": {
        "id": "T2xnwdvdlKyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Care-GNN training\n",
        "features = nn.Embedding(feat_data.shape[0], feat_data.shape[1])\n",
        "feat_data = normalize(feat_data)\n",
        "features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)\n",
        "if cuda:\n",
        "    features.cuda()\n",
        "\n",
        "adj_lists = [relation1, relation2, relation3]\n",
        "\n",
        "# build the model\n",
        "intra1 = IntraAgg(features, feat_data.shape[1], cuda=cuda)\n",
        "intra2 = IntraAgg(features, feat_data.shape[1], cuda=cuda)\n",
        "intra3 = IntraAgg(features, feat_data.shape[1], cuda=cuda)\n",
        "inter1 = InterAgg(features, feat_data.shape[1], emb_size, adj_lists, [intra1, intra2, intra3], inter=inter,\n",
        "                    step_size=step_size, cuda=cuda)\n",
        "gnn_model = OneLayerCARE(2, inter1, lambda_1)\n",
        "\n",
        "if cuda:\n",
        "  gnn_model.cuda()\n",
        "\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, gnn_model.parameters()), lr=lr, weight_decay=lambda_2)\n",
        "\n",
        "performance_log = []\n",
        "# train the model\n",
        "print(f'Training nodes: {len(X_train)}')\n",
        "for epoch in range(30):\n",
        "  # send number of batches to model to let the RLModule know the training progress\n",
        "  inter1.batch_num = 1 #Equal to 1 because no mini-batch training\n",
        "    \n",
        "  optimizer.zero_grad()\n",
        "  if cuda:\n",
        "    loss = gnn_model.loss(X_train, Variable(torch.cuda.LongTensor(y_train)))\n",
        "  else:\n",
        "    loss = gnn_model.loss(X_train, Variable(torch.LongTensor(y_train)))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(f'Epoch: {epoch}, loss: {loss.item() / num_batches}')"
      ],
      "metadata": {
        "id": "CPANws3786ni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "393b4282-b0f7-4c98-b95a-a49026d9738e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training nodes: 10800\n",
            "Epoch: 0, loss: 0.7939790089925131\n",
            "Epoch: 1, loss: 0.7415876388549805\n",
            "epoch scores: [0.044845915884961014, 0.03362197244169051, 0.02831463308126549]\n",
            "rewards: [-1, -1, 1]\n",
            "thresholds: [0.48, 0.48, 0.52]\n",
            "Epoch: 2, loss: 0.6948471069335938\n",
            "epoch scores: [0.04483712153481518, 0.03361620013488305, 0.02831135755831665]\n",
            "rewards: [1, 1, 1]\n",
            "thresholds: [0.5, 0.5, 0.54]\n",
            "Epoch: 3, loss: 0.6539614597956339\n",
            "epoch scores: [0.044067511305934025, 0.0327327867737886, 0.030278614244731328]\n",
            "rewards: [1, 1, -1]\n",
            "thresholds: [0.52, 0.52, 0.52]\n",
            "Epoch: 4, loss: 0.615675171216329\n",
            "epoch scores: [0.044760477206601194, 0.03356624467498198, 0.031156347028636296]\n",
            "rewards: [-1, -1, -1]\n",
            "thresholds: [0.5, 0.5, 0.5]\n",
            "Epoch: 5, loss: 0.5825599431991577\n",
            "epoch scores: [0.04607620833417217, 0.03455738643343571, 0.03022681219606561]\n",
            "rewards: [-1, -1, 1]\n",
            "thresholds: [0.48, 0.48, 0.52]\n",
            "Epoch: 6, loss: 0.5548523267110189\n",
            "epoch scores: [0.044533125776288617, 0.03341839957654582, 0.02820455473632984]\n",
            "rewards: [1, 1, 1]\n",
            "thresholds: [0.5, 0.5, 0.54]\n",
            "Epoch: 7, loss: 0.5301417112350464\n",
            "epoch scores: [0.04358888920340879, 0.03242024989353324, 0.03009996911274345]\n",
            "rewards: [1, 1, -1]\n",
            "thresholds: [0.52, 0.52, 0.52]\n",
            "Epoch: 8, loss: 0.5069777965545654\n",
            "epoch scores: [0.04403675009464478, 0.03309601892434299, 0.030887802045388338]\n",
            "rewards: [-1, -1, -1]\n",
            "thresholds: [0.5, 0.5, 0.5]\n",
            "Epoch: 9, loss: 0.4880181550979614\n",
            "epoch scores: [0.04502697788370293, 0.03387931018820004, 0.02984723521814007]\n",
            "rewards: [-1, -1, 1]\n",
            "thresholds: [0.48, 0.48, 0.52]\n",
            "Epoch: 10, loss: 0.4722761313120524\n",
            "epoch scores: [0.043108719987316774, 0.03249406430418434, 0.027709211718315727]\n",
            "rewards: [1, 1, 1]\n",
            "thresholds: [0.5, 0.5, 0.54]\n",
            "Epoch: 11, loss: 0.4581034978230794\n",
            "epoch scores: [0.04170236085280537, 0.031191004548432946, 0.029400853435299676]\n",
            "rewards: [1, 1, -1]\n",
            "thresholds: [0.52, 0.52, 0.52]\n",
            "Epoch: 12, loss: 0.445236603418986\n",
            "epoch scores: [0.0415389908727733, 0.03147752205801524, 0.029966717224666782]\n",
            "rewards: [1, -1, -1]\n",
            "thresholds: [0.54, 0.5, 0.5]\n",
            "Epoch: 13, loss: 0.4339134693145752\n",
            "epoch scores: [0.04178007113329099, 0.031785847710303766, 0.028677822861435085]\n",
            "rewards: [-1, -1, 1]\n",
            "thresholds: [0.52, 0.48, 0.52]\n",
            "Epoch: 14, loss: 0.42292988300323486\n",
            "epoch scores: [0.041217105516785466, 0.029895123059959516, 0.026316963768734384]\n",
            "rewards: [1, 1, 1]\n",
            "thresholds: [0.54, 0.5, 0.54]\n",
            "Epoch: 15, loss: 0.4126058022181193\n",
            "epoch scores: [0.03876258065901938, 0.028015918256617325, 0.02759151146119478]\n",
            "rewards: [1, 1, -1]\n",
            "thresholds: [0.56, 0.52, 0.52]\n",
            "Epoch: 16, loss: 0.40319828192392987\n",
            "epoch scores: [0.03757883292830437, 0.027616254259555245, 0.02776116394868865]\n",
            "rewards: [1, 1, -1]\n",
            "thresholds: [0.5800000000000001, 0.54, 0.5]\n",
            "Epoch: 17, loss: 0.39373751481374103\n",
            "epoch scores: [0.03608875088017683, 0.027164323498427352, 0.026080479103849974]\n",
            "rewards: [1, 1, 1]\n",
            "thresholds: [0.6000000000000001, 0.56, 0.52]\n",
            "Epoch: 18, loss: 0.3841610749562581\n",
            "epoch scores: [0.03414412321617583, 0.02643941017204877, 0.023448920773248658]\n",
            "rewards: [1, 1, 1]\n",
            "thresholds: [0.6200000000000001, 0.5800000000000001, 0.54]\n",
            "Epoch: 19, loss: 0.37548057238260907\n",
            "epoch scores: [0.03213916265497954, 0.02555180029095123, 0.024142423450028136]\n",
            "rewards: [1, 1, -1]\n",
            "thresholds: [0.6400000000000001, 0.6000000000000001, 0.52]\n",
            "Epoch: 20, loss: 0.3665965795516968\n",
            "epoch scores: [0.029483024232334294, 0.024495853328200073, 0.023866740102124546]\n",
            "rewards: [1, 1, 1]\n",
            "thresholds: [0.6600000000000001, 0.6200000000000001, 0.54]\n",
            "Epoch: 21, loss: 0.35728339354197186\n",
            "epoch scores: [0.026683618961037287, 0.02337009263234544, 0.021860624777392534]\n",
            "rewards: [1, 1, 1]\n",
            "thresholds: [0.6800000000000002, 0.6400000000000001, 0.56]\n",
            "Epoch: 22, loss: 0.34840643405914307\n",
            "epoch scores: [0.023693168207625185, 0.022144251048964458, 0.02146950799121532]\n",
            "rewards: [1, 1, 1]\n",
            "thresholds: [0.7000000000000002, 0.6600000000000001, 0.5800000000000001]\n",
            "Epoch: 23, loss: 0.33955836296081543\n",
            "epoch scores: [0.020945482041225115, 0.021019621607421567, 0.021054201650271026]\n",
            "rewards: [1, 1, 1]\n",
            "thresholds: [0.7200000000000002, 0.6800000000000002, 0.6000000000000001]\n",
            "Epoch: 24, loss: 0.330723504225413\n",
            "epoch scores: [0.018650528621742962, 0.02015714015501458, 0.020673244118185368]\n",
            "rewards: [1, 1, 1]\n",
            "thresholds: [0.7400000000000002, 0.7000000000000002, 0.6200000000000001]\n",
            "Epoch: 25, loss: 0.3219417929649353\n",
            "epoch scores: [0.017655361968962873, 0.01986143316512845, 0.02077754616197652]\n",
            "rewards: [1, 1, -1]\n",
            "thresholds: [0.7600000000000002, 0.7200000000000002, 0.6000000000000001]\n",
            "Epoch: 26, loss: 0.3132344086964925\n",
            "epoch scores: [0.018025838474740773, 0.020244735530481776, 0.020544895090395893]\n",
            "rewards: [-1, -1, 1]\n",
            "thresholds: [0.7400000000000002, 0.7000000000000002, 0.6200000000000001]\n",
            "Epoch: 27, loss: 0.30463387568791706\n",
            "epoch scores: [0.019717887857700563, 0.021240048604122645, 0.019333378079251912]\n",
            "rewards: [-1, -1, 1]\n",
            "thresholds: [0.7200000000000002, 0.6800000000000002, 0.6400000000000001]\n",
            "Epoch: 28, loss: 0.2962891260782878\n",
            "epoch scores: [0.02141829977166499, 0.02110889998926242, 0.01943593107429881]\n",
            "rewards: [-1, 1, -1]\n",
            "thresholds: [0.7000000000000002, 0.7000000000000002, 0.6200000000000001]\n",
            "Epoch: 29, loss: 0.28879159688949585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict using CARE-GNN\n",
        "#_, _, _, _,predicted_labels,predict_prob = test_care(X_test, y_test, gnn_model, batch_size)\n",
        "gnn_prob, label_prob = gnn_model.to_prob(X_test, y_test, train_flag=False)\n",
        "care_label=gnn_prob.data.cpu().numpy().argmax(axis=1)"
      ],
      "metadata": {
        "id": "-xF1HmtJ86p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get true labels for nodes predicted in ensemble models and care-gnn\n",
        "true_before=np.array([label_origin[i] for i in X_train[4777:]])#The first 4777 nodes in X_train are original labeled training nodes\n",
        "true_remain=np.array([label_origin[i] for i in X_test])#Remaining nodes are predicted by care-gnn\n",
        "final_true_all=np.append(true_before,true_remain)\n",
        "#Get all predicted labels\n",
        "final_predict_all=np.append(pseudo_labels,care_label)"
      ],
      "metadata": {
        "id": "iS-jGXSR86sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(final_true_all,final_predict_all))"
      ],
      "metadata": {
        "id": "2_3mSlm0869r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c620384-a881-4630-f79d-684eb448e366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.95      1.00      0.98      6674\n",
            "         1.0       0.97      0.34      0.50       493\n",
            "\n",
            "    accuracy                           0.95      7167\n",
            "   macro avg       0.96      0.67      0.74      7167\n",
            "weighted avg       0.95      0.95      0.94      7167\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_probs=np.append(probs,gnn_prob.data.cpu().numpy(),axis=0)"
      ],
      "metadata": {
        "id": "8wDNQMb6y3Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(roc_auc_score(final_true_all, final_probs[:,1]))"
      ],
      "metadata": {
        "id": "4vTo7IVu86_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70e29b8f-c6a0-4853-b388-3e1005522f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9083570952277039\n"
          ]
        }
      ]
    }
  ]
}